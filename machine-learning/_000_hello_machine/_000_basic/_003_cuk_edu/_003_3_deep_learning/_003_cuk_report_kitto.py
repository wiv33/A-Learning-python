# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fdpagSr11paf_0klMPhxWcG6tofmorXo
"""

import numpy as np
from keras.layers import Dense, Dropout, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D
from keras.models import Sequential, load_model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical

(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

img_gen = ImageDataGenerator(
    rotation_range=15,
    zoom_range=0.1,
    shear_range=0.6,
    width_shift_range=0.15,
    height_shift_range=0.1,
    horizontal_flip=True,
    vertical_flip=False,
    # validation_split=0.2
)


def augments(gen, np_img, np_answer, augment_size=10):
    arr_size = len(np_img) * augment_size
    result = [] * arr_size
    result_answer = [] * arr_size
    for x in range(len(np_img)):
        gen_result = gen.flow(np.tile(np_img[x].reshape(28 * 28), augment_size).reshape(-1, 28, 28, 1),
                              np.zeros(augment_size),
                              batch_size=augment_size,
                              shuffle=False).next()[0]
        answer = np_answer[x]
        for j in gen_result:
            result.append(j)
            result_answer.append(answer)

    return np.array(result), np.array(result_answer)


x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)

augment_size = 90000
random_mask = np.random.randint(x_train.shape[0], size=augment_size)
x_augmented = x_train[random_mask].copy()
y_augmented = y_train[random_mask].copy()
x_augmented = img_gen.flow(x_augmented,
                           np.zeros(augment_size),
                           batch_size=augment_size,
                           shuffle=False).next()[0]

x_train = np.concatenate((x_train, x_augmented))
y_train = np.concatenate((y_train, y_augmented))

print(x_train.shape)
print(y_train.shape)


# x_res, y_res = augments(img_gen, x_train, y_train)
# x_res.shape, y_res.shape

def run_gen(gen, x):
    augment_size = len(x)
    return gen.flow(x, np.zeros(augment_size), batch_size=augment_size, shuffle=False).next()[0]


def extract(x, y, target):
    t = np.where(y == target)
    x_result = x[t].copy()
    y_result = y[t].copy()
    print(len(x_result), len(y_result))
    return x_result, y_result


def augment_v2(gen, np_img, np_answer):
    x_top, y_top = extract(np_img, np_answer, 0)
    x_pullover, y_pullover = extract(np_img, np_answer, 2)
    x_coat, y_coat = extract(np_img, np_answer, 4)
    x_shirt, y_shirt = extract(np_img, np_answer, 6)
    x_shirt2, y_shirt2 = extract(np_img, np_answer, 6)

    print("정상 1")
    tops = run_gen(gen, x_top)
    pullover = run_gen(gen, x_pullover)
    coats = run_gen(gen, x_coat)
    shirts = run_gen(gen, x_shirt)
    shirts2 = run_gen(gen, x_shirt2)

    print("정상 2")
    np_img = np.concatenate((np_img, tops))
    np_img = np.concatenate((np_img, pullover))
    np_img = np.concatenate((np_img, coats))
    np_img = np.concatenate((np_img, shirts))
    np_img = np.concatenate((np_img, shirts2))

    print("x_train size : ", len(np_img))
    print("정상 3")
    np_answer = np.concatenate((np_answer, y_top))
    np_answer = np.concatenate((np_answer, y_pullover))
    np_answer = np.concatenate((np_answer, y_coat))
    np_answer = np.concatenate((np_answer, y_shirt))
    np_answer = np.concatenate((np_answer, y_shirt2))

    print("y_train size : ", len(np_answer))
    return np_img.copy(), np_answer.copy()


x_train, y_train = augment_v2(img_gen, x_train, y_train)

x_train, y_train = augments(img_gen, x_train, y_train)
x_eval, y_eval = augments(img_gen, x_test, y_test)

x_train.shape, y_train.shape


def my_model(filters=[32, 64], kernel_size=(3, 3), pool_size=(2, 2), act='relu'):
    width, height, depth, classes = 28, 28, 1, 10

    model = Sequential()
    input_shape = (height, width, depth)
    chan_dim = -1

    for i, x in enumerate(filters):
        if i == 0:
            model.add(Conv2D(filters=x,
                             kernel_size=kernel_size,
                             padding='same',
                             input_shape=input_shape))
        else:
            model.add(Conv2D(filters=x, kernel_size=kernel_size, padding='same'))

        model.add(Conv2D(filters=x, kernel_size=kernel_size, padding='same'))
        model.add(BatchNormalization(axis=chan_dim))
        model.add(Activation(act))
        model.add(Conv2D(filters=x, kernel_size=kernel_size, padding='same'))
        model.add(Conv2D(filters=x, kernel_size=kernel_size, padding='same'))
        model.add(BatchNormalization(axis=chan_dim))
        model.add(Activation(act))
        model.add(MaxPooling2D(pool_size=pool_size))
        model.add(Dropout(0.3))

    model.add(Flatten())

    model.add(Dense(256))
    model.add(BatchNormalization())
    model.add(Activation(act))
    model.add(Dropout(0.4))

    model.add(Dense(64))
    model.add(BatchNormalization())
    model.add(Activation(act))
    model.add(Dropout(0.4))

    model.add(Dense(classes))
    model.add(Activation('softmax'))

    return model


# x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
# x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)

x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.

classes = len(np.unique(y_train))
y_train = to_categorical(y_train, num_classes=classes)
y_test = to_categorical(y_test, num_classes=classes)

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

bat_size = 128
epochs = 50
lr_arr = [0.001]
lr_size = len(lr_arr)
# 모델 저장 기준이 되는 콜백 import

for i, (lr, bs, ep) in enumerate(zip(lr_arr, [bat_size] * lr_size, [epochs] * lr_size)):
    # 체크할 포인트
    check_point = ModelCheckpoint(f'best_model_{i}.h5',
                                  monitor='val_accuracy',
                                  verbose=1,
                                  save_best_only=True)

    # 이른 스탑을 위한 클래스
    early_stopping = EarlyStopping(monitor='val_accruacy',
                                   min_delta=0,
                                   patience=7,
                                   verbose=1)
    optimizer = Adam(learning_rate=lr,
                     decay=lr / ep,
                     beta_1=0.9,
                     beta_2=0.999)

    model = my_model()

    model.compile(optimizer=optimizer,
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    history = model.fit(x_train,
                        y_train,
                        batch_size=bs,
                        validation_split=0.3,
                        validation_data=(x_eval, y_eval),
                        epochs=ep,
                        verbose=2,
                        callbacks=[check_point, early_stopping])

for x in range(1):
    loaded_model = load_model(f'best_model_{x}.h5')

    prob_pred = loaded_model.predict(x_test)
    pred = loaded_model.evaluate(x_test, y_test)
    print(pred)

    prob_label = prob_pred.argmax(axis=-1)

    np.savetxt(f'y_pred_{x}.csv', prob_label, fmt='%d')

# show a nicely formatted classification report
from sklearn.metrics import classification_report

label_names = ["top", "trouser", "pullover", "dress", "coat", "sandal", "shirt", "sneaker", "bag", "ankle boot"]

print("[INFO] evaluating network...")
print(classification_report(y_test.argmax(axis=1), prob_pred.argmax(axis=1), target_names=label_names))
