{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09_yolo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wiv33/A-Learning-python/blob/master/machine-learning/_000_hello_machine/_000_basic/_003_cuk_edu/_005_3_computer_vision/09_yolo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUAzXte9t5vP"
      },
      "source": [
        "# Errata\n",
        "\n",
        "1. 1차시 강의 중 최대 객체 수 오류 정정  \n",
        "   1차시 30:19 경에 YOLO의 예를 들면서 grid가 19x19이고 Anchor가 9개이고 class가 80개인 경우 27만개라고 한 부분은 잘못된 설명입니다. (27만개는 단순한 텐서의 크기입니다.)  \n",
        "   감지할 수 있는 총 객체의 수는 = grid x grid x anchor 수 만큼입니다.  \n",
        "   따라서 한 이미지에서 YOLO의 기본적인 최대 객체 감지 수는 19x19x9 = 3249개 입니다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6o00Pc8lvZg"
      },
      "source": [
        "# Batch normalization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMNXnUzrlxqk"
      },
      "source": [
        "gdown upgrade"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z7BoduGF3Hb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "741a7dae-6615-4f93-a147-e8fc32b0639a"
      },
      "source": [
        "!pip install --upgrade gdown"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPEA2YkSl0sk"
      },
      "source": [
        "구글 드라이브 마운트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9aHZRzYGJIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7db008a7-b18c-4dbf-e29d-857a45f2243c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOhF5eqtmnhA"
      },
      "source": [
        "**Batch normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gv0BDD7hlrq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a187f12e-2ee0-4bb5-c7ec-14d9e1a4c970"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import  Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = keras.Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 576)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                36928     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTf6ZQ4HiEQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75328163-bc29-4f8c-ced9-7079b2a447c9"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation\n",
        "\n",
        "model = keras.Sequential([\n",
        "    Conv2D(32, (3,3), input_shape=(28, 28, 1)),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Conv2D(64, (3,3)),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Conv2D(64, (3,3)),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 26, 26, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 26, 26, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 13, 13, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 11, 11, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 11, 11, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 3, 3, 64)          36928     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 3, 3, 64)         256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 3, 3, 64)          0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 576)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                36928     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 93,962\n",
            "Trainable params: 93,642\n",
            "Non-trainable params: 320\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqkX80jVRJWJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4c54b29-bb60-489c-af85-f1be430fb199"
      },
      "source": [
        "93962-93322"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "640"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UPVSX3nidav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f638488-eea6-468c-fe8e-e23cf227a2b9"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation\n",
        "\n",
        "# 기존 \n",
        "Conv2D(64, (3,3), activation='relu'),\n",
        "\n",
        "# 기존과 동일 코드\n",
        "Conv2D(64, (3,3)),\n",
        "Activation('relu'),\n",
        "\n",
        "# Batch Normalization\n",
        "Conv2D(64, (3,3)),\n",
        "BatchNormalization(),\n",
        "Activation('relu'),\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<keras.layers.core.Activation at 0x7f39201dfd90>,)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kELrYhdFp8Z"
      },
      "source": [
        "# Keras-YOLO  \n",
        "\n",
        "공개 소스(open source) 기반의 대규모 소스를 한번에 이해하기는 어렵습니다.  \n",
        "특히 이번 yolo 소스의 경우에는 전체 소스가 google colab에서 보이지 않습니다.  \n",
        "전체 소스를 이해하기 위해서는 GIT에서 전체 소스를 다운받아서 천천히 살펴 보아야 합니다.\n",
        "\n",
        "우선적으로 다음과 같은 순서로 소스를 살펴보도록 합시다.\n",
        "\n",
        "1. 소스의 구동  \n",
        "   오픈 소스의 경우 구동 자체가 어려운 경우가 많습니다.  \n",
        "   차근차근 실행해 보면서 전체의 구동 구조를 파악합니다.\n",
        "\n",
        "2. 데이터 형식의 이해  \n",
        "   입출력 데이터 형식을 이해하려고 노력해 봅시다.  \n",
        "   객체 탐지의 경우 프로젝트마다 유사점은 있지만, 각각의 독립적인 데이터 형식들을 취하고 있습니다.  \n",
        "   입출력 데이터의 형식을 파악하는 것이 중요합니다.\n",
        "\n",
        "3. 소스의 이해  \n",
        "   소스를 파악할 때, 세부사항보다는 전체의 함수 흐름을 먼저 이해해 봅시다.  \n",
        "   당장은 이해가 안가더라도, 소스를 구동하고 각각의 변수를 출력해가면서 분석하다 보면,  \n",
        "   점차적으로 이해되는 경우가 많습니다.  \n",
        "\n",
        "**이번 실습의 목표**  \n",
        "\n",
        "본 실습의 목표는 소스의 대략적인 구조의 이해와 구동을 성공시키는 것입니다.  \n",
        "그리고 강의에서 배운 YOLO의 구조들이 소스의 어느 부분에 위치하는 지를 파악해 봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7_PEQynGoL-"
      },
      "source": [
        "**Source code repo**  \n",
        "https://github.com/shevious/keras-yolo4.git  \n",
        "\n",
        "**Original source code repo**  \n",
        "https://github.com/Ma-Dan/keras-yolo4  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8g6hjNUFrxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b87a932e-5bf9-4a99-d82d-5bc1a5081ba5"
      },
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/shevious/keras-yolo4.git"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'keras-yolo4'...\n",
            "remote: Enumerating objects: 96, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 96 (delta 8), reused 12 (delta 4), pack-reused 72\u001b[K\n",
            "Unpacking objects: 100% (96/96), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYf8mHNrGmH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b06d66-70a5-4513-c99c-8649f575a2c6"
      },
      "source": [
        "%cd /content/keras-yolo4"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/keras-yolo4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mfnwc4B3X8gH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e7c74e5a-8db2-4079-a246-abbf5e409f82"
      },
      "source": [
        "import gdown\n",
        "md5 = '6cd6e144f989b92b3379bac3b3de84fd'\n",
        "#url = 'https://drive.google.com/file/d/14eyB2lkxrG83fGy4t395sgZqADOYJ_yE/view?usp=sharing'\n",
        "url = 'https://drive.google.com/uc?id=14eyB2lkxrG83fGy4t395sgZqADOYJ_yE'\n",
        "output = '/mydrive/VOCtrainval_11-May-2012.tar'\n",
        "gdown.cached_download(url, output, md5=md5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing MD5: /mydrive/VOCtrainval_11-May-2012.tar\n",
            "MD5 matches: /mydrive/VOCtrainval_11-May-2012.tar\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/mydrive/VOCtrainval_11-May-2012.tar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CO65-vnG2gi"
      },
      "source": [
        "!tar xf /mydrive/VOCtrainval_11-May-2012.tar"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raqmbVOOHZY9"
      },
      "source": [
        "!ln -s VOCdevkit data"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9Y5v-d-G8Nr"
      },
      "source": [
        "!python voc_annotation.py"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjg6ngIxTia4"
      },
      "source": [
        "**import section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaQx3wfTHwnQ"
      },
      "source": [
        "from functools import wraps\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import Conv2D, Add, ZeroPadding2D, UpSampling2D, Concatenate, MaxPooling2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "from yolo4.utils import compose\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbEYG9TxTnN3"
      },
      "source": [
        "**Mish activation function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV47bT0TTp1L"
      },
      "source": [
        "class Mish(Layer):\n",
        "    '''\n",
        "    Mish Activation Function.\n",
        "    .. math::\n",
        "        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n",
        "    Shape:\n",
        "        - Input: Arbitrary. Use the keyword argument `input_shape`\n",
        "        (tuple of integers, does not include the samples axis)\n",
        "        when using this layer as the first layer in a model.\n",
        "        - Output: Same shape as the input.\n",
        "    Examples:\n",
        "        >>> X_input = Input(input_shape)\n",
        "        >>> X = Mish()(X_input)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Mish, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs * K.tanh(K.softplus(inputs))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Mish, self).get_config()\n",
        "        return config\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tNKpQSGTumC"
      },
      "source": [
        "**CSPDarknet53 backbone**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N4VsT-6Txkx"
      },
      "source": [
        "@wraps(Conv2D) # comment, document를 그대로 사용하는 의미\n",
        "def DarknetConv2D(*args, **kwargs):\n",
        "    \"\"\"Wrapper to set Darknet parameters for Convolution2D.\"\"\"\n",
        "    darknet_conv_kwargs = {}\n",
        "    darknet_conv_kwargs['kernel_initializer'] = keras.initializers.RandomNormal(mean=0.0, stddev=0.01)\n",
        "    darknet_conv_kwargs['padding'] = 'valid' if kwargs.get('strides')==(2,2) else 'same'\n",
        "    darknet_conv_kwargs.update(kwargs)\n",
        "    return Conv2D(*args, **darknet_conv_kwargs)\n",
        "\n",
        "def DarknetConv2D_BN_Leaky(*args, **kwargs):\n",
        "    \"\"\"Darknet Convolution2D followed by BatchNormalization and LeakyReLU.\"\"\"\n",
        "    no_bias_kwargs = {'use_bias': False}\n",
        "    no_bias_kwargs.update(kwargs)\n",
        "    return compose(\n",
        "        DarknetConv2D(*args, **no_bias_kwargs),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.1)) # alpha = leaky 왼쪽 부분의 기울기\n",
        "\n",
        "def DarknetConv2D_BN_Mish(*args, **kwargs):\n",
        "    \"\"\"Darknet Convolution2D followed by BatchNormalization and LeakyReLU.\"\"\"\n",
        "    no_bias_kwargs = {'use_bias': False}\n",
        "    no_bias_kwargs.update(kwargs)\n",
        "    return compose(\n",
        "        DarknetConv2D(*args, **no_bias_kwargs),\n",
        "        BatchNormalization(),\n",
        "        Mish())\n",
        "\n",
        "# 위 함수들을 합친 body\n",
        "# cross stage partial \n",
        "# backbone\n",
        "def resblock_body(x, num_filters, num_blocks, all_narrow=True):\n",
        "    '''A series of resblocks starting with a downsampling Convolution2D'''\n",
        "    # Darknet uses left and top padding instead of 'same' mode\n",
        "    preconv1 = ZeroPadding2D(((1,0),(1,0)))(x)\n",
        "    preconv1 = DarknetConv2D_BN_Mish(num_filters, (3,3), strides=(2,2))(preconv1)\n",
        "    \n",
        "    shortconv = DarknetConv2D_BN_Mish(num_filters//2 if all_narrow else num_filters, (1,1))(preconv1)\n",
        "    mainconv = DarknetConv2D_BN_Mish(num_filters//2 if all_narrow else num_filters, (1,1))(preconv1)\n",
        "    # skip connection\n",
        "    for i in range(num_blocks):\n",
        "        y = compose(\n",
        "                DarknetConv2D_BN_Mish(num_filters//2, (1,1)),\n",
        "                DarknetConv2D_BN_Mish(num_filters//2 if all_narrow else num_filters, (3,3)))(mainconv)\n",
        "        mainconv = Add()([mainconv,y])\n",
        "    postconv = DarknetConv2D_BN_Mish(num_filters//2 if all_narrow else num_filters, (1,1))(mainconv)\n",
        "    \n",
        "    route = Concatenate()([postconv, shortconv])\n",
        "    return DarknetConv2D_BN_Mish(num_filters, (1,1))(route)\n",
        "\n",
        "def darknet_body(x):\n",
        "    '''Darknent body having 52 Convolution2D layers'''\n",
        "    x = DarknetConv2D_BN_Mish(32, (3,3))(x)\n",
        "    x = resblock_body(x, 64, 1, False)\n",
        "    x = resblock_body(x, 128, 2)\n",
        "    x = resblock_body(x, 256, 8)\n",
        "    x = resblock_body(x, 512, 8)\n",
        "    x = resblock_body(x, 1024, 4)\n",
        "    return x"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-uLv4aUT7iw"
      },
      "source": [
        "**YOLO body**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AsC8JLeT9Va"
      },
      "source": [
        "# neck, head 로 불리우는 부분\n",
        "def yolo4_body(inputs, num_anchors, num_classes):\n",
        "    \"\"\"Create YOLO_V4 model CNN body in Keras.\"\"\"\n",
        "    darknet = Model(inputs, darknet_body(inputs))\n",
        "\n",
        "    #19x19 head /// 원 이미지의 1/32로 줄었다는 의미\n",
        "    y19 = DarknetConv2D_BN_Leaky(512, (1,1))(darknet.output)\n",
        "    y19 = DarknetConv2D_BN_Leaky(1024, (3,3))(y19)\n",
        "    y19 = DarknetConv2D_BN_Leaky(512, (1,1))(y19)\n",
        "    # SPP - Spatial Pyramid Pooling\n",
        "    maxpool1 = MaxPooling2D(pool_size=(13,13), strides=(1,1), padding='same')(y19)\n",
        "    maxpool2 = MaxPooling2D(pool_size=(9,9), strides=(1,1), padding='same')(y19)\n",
        "    maxpool3 = MaxPooling2D(pool_size=(5,5), strides=(1,1), padding='same')(y19)\n",
        "    y19 = Concatenate()([maxpool1, maxpool2, maxpool3, y19])\n",
        "\n",
        "    y19 = DarknetConv2D_BN_Leaky(512, (1,1))(y19)\n",
        "    y19 = DarknetConv2D_BN_Leaky(1024, (3,3))(y19)\n",
        "    y19 = DarknetConv2D_BN_Leaky(512, (1,1))(y19)\n",
        "\n",
        "    y19_upsample = compose(DarknetConv2D_BN_Leaky(256, (1,1)), UpSampling2D(2))(y19)\n",
        "\n",
        "    #38x38 head\n",
        "    y38 = DarknetConv2D_BN_Leaky(256, (1,1))(darknet.layers[204].output)\n",
        "    y38 = Concatenate()([y38, y19_upsample])\n",
        "    y38 = DarknetConv2D_BN_Leaky(256, (1,1))(y38)\n",
        "    y38 = DarknetConv2D_BN_Leaky(512, (3,3))(y38)\n",
        "    y38 = DarknetConv2D_BN_Leaky(256, (1,1))(y38)\n",
        "    y38 = DarknetConv2D_BN_Leaky(512, (3,3))(y38)\n",
        "    y38 = DarknetConv2D_BN_Leaky(256, (1,1))(y38)\n",
        "\n",
        "    y38_upsample = compose(DarknetConv2D_BN_Leaky(128, (1,1)), UpSampling2D(2))(y38)\n",
        "\n",
        "    #76x76 head\n",
        "    y76 = DarknetConv2D_BN_Leaky(128, (1,1))(darknet.layers[131].output)\n",
        "    y76 = Concatenate()([y76, y38_upsample])\n",
        "    y76 = DarknetConv2D_BN_Leaky(128, (1,1))(y76)\n",
        "    y76 = DarknetConv2D_BN_Leaky(256, (3,3))(y76)\n",
        "    y76 = DarknetConv2D_BN_Leaky(128, (1,1))(y76)\n",
        "    y76 = DarknetConv2D_BN_Leaky(256, (3,3))(y76)\n",
        "    y76 = DarknetConv2D_BN_Leaky(128, (1,1))(y76)\n",
        "\n",
        "    # PAN - path aggregation network\n",
        "    #76x76 output\n",
        "    y76_output = DarknetConv2D_BN_Leaky(256, (3,3))(y76)\n",
        "    y76_output = DarknetConv2D(num_anchors*(num_classes+5), (1,1))(y76_output)\n",
        "\n",
        "    #38x38 output\n",
        "    y76_downsample = ZeroPadding2D(((1,0),(1,0)))(y76)\n",
        "    y76_downsample = DarknetConv2D_BN_Leaky(256, (3,3), strides=(2,2))(y76_downsample)\n",
        "    y38 = Concatenate()([y76_downsample, y38])\n",
        "    y38 = DarknetConv2D_BN_Leaky(256, (1,1))(y38)\n",
        "    y38 = DarknetConv2D_BN_Leaky(512, (3,3))(y38)\n",
        "    y38 = DarknetConv2D_BN_Leaky(256, (1,1))(y38)\n",
        "    y38 = DarknetConv2D_BN_Leaky(512, (3,3))(y38)\n",
        "    y38 = DarknetConv2D_BN_Leaky(256, (1,1))(y38)\n",
        "\n",
        "    y38_output = DarknetConv2D_BN_Leaky(512, (3,3))(y38)\n",
        "    y38_output = DarknetConv2D(num_anchors*(num_classes+5), (1,1))(y38_output)\n",
        "\n",
        "    #19x19 output\n",
        "    y38_downsample = ZeroPadding2D(((1,0),(1,0)))(y38)\n",
        "    y38_downsample = DarknetConv2D_BN_Leaky(512, (3,3), strides=(2,2))(y38_downsample)\n",
        "    y19 = Concatenate()([y38_downsample, y19])\n",
        "    y19 = DarknetConv2D_BN_Leaky(512, (1,1))(y19)\n",
        "    y19 = DarknetConv2D_BN_Leaky(1024, (3,3))(y19)\n",
        "    y19 = DarknetConv2D_BN_Leaky(512, (1,1))(y19)\n",
        "    y19 = DarknetConv2D_BN_Leaky(1024, (3,3))(y19)\n",
        "    y19 = DarknetConv2D_BN_Leaky(512, (1,1))(y19)\n",
        "\n",
        "    y19_output = DarknetConv2D_BN_Leaky(1024, (3,3))(y19)\n",
        "    y19_output = DarknetConv2D(num_anchors*(num_classes+5), (1,1))(y19_output)\n",
        "\n",
        "    # 19=large, 38=medium, 76=small\n",
        "    yolo4_model = Model(inputs, [y19_output, y38_output, y76_output])\n",
        "\n",
        "    return yolo4_model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LKzbIRsYszd"
      },
      "source": [
        "**YOLO loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR-SmCpQYu9n"
      },
      "source": [
        "def bbox_ciou(boxes1, boxes2):\n",
        "    '''\n",
        "    caluate ciou = iou - p2/c2 - av\n",
        "    : (batch_size, grid_r, grid_c, anchor, box)\n",
        "    :param boxes1: (8, 13, 13, 3, 4)   pred_xywh\n",
        "    :param boxes2: (8, 13, 13, 3, 4)   label_xywh\n",
        "    :return:\n",
        "\n",
        "    For example, assume that the shapes of pred_xywh and label_xywh are both (1, 4)\n",
        "    '''\n",
        "\n",
        "    # convert to the upper left corner coordinate, the lower right corner coordinate\n",
        "    boxes1_x0y0x1y1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n",
        "                                 boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n",
        "    boxes2_x0y0x1y1 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n",
        "                                 boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n",
        "    '''\n",
        "    Compare boxes1_x0y0x1y1[..., :2] and boxes1_x0y0x1y1[..., 2:] position by position, that is, compare [x0, y0] and [x1, y1] position by position, leaving the smaller ones.\n",
        "    For example, leaving [x0, y0]\n",
        "    This step is to avoid that w h is a negative number at the beginning, causing x0y0 to become the coordinates of the lower right corner and x1y1 to become the coordinates of the upper left corner.\n",
        "    '''\n",
        "    boxes1_x0y0x1y1 = tf.concat([tf.minimum(boxes1_x0y0x1y1[..., :2], boxes1_x0y0x1y1[..., 2:]),\n",
        "                                 tf.maximum(boxes1_x0y0x1y1[..., :2], boxes1_x0y0x1y1[..., 2:])], axis=-1)\n",
        "    boxes2_x0y0x1y1 = tf.concat([tf.minimum(boxes2_x0y0x1y1[..., :2], boxes2_x0y0x1y1[..., 2:]),\n",
        "                                 tf.maximum(boxes2_x0y0x1y1[..., :2], boxes2_x0y0x1y1[..., 2:])], axis=-1)\n",
        "\n",
        "    # The area of the two rectangles\n",
        "    boxes1_area = (boxes1_x0y0x1y1[..., 2] - boxes1_x0y0x1y1[..., 0]) * (\n",
        "                boxes1_x0y0x1y1[..., 3] - boxes1_x0y0x1y1[..., 1])\n",
        "    boxes2_area = (boxes2_x0y0x1y1[..., 2] - boxes2_x0y0x1y1[..., 0]) * (\n",
        "                boxes2_x0y0x1y1[..., 3] - boxes2_x0y0x1y1[..., 1])\n",
        "\n",
        "    # The coordinates of the upper left corner and the lower right corner of the intersecting rectangle, the shapes are both (8, 13, 13, 3, 2)\n",
        "    left_up = tf.maximum(boxes1_x0y0x1y1[..., :2], boxes2_x0y0x1y1[..., :2])\n",
        "    right_down = tf.minimum(boxes1_x0y0x1y1[..., 2:], boxes2_x0y0x1y1[..., 2:])\n",
        "\n",
        "    # The area of the intersecting rectangle inter_area. iou\n",
        "    inter_section = tf.maximum(right_down - left_up, 0.0)\n",
        "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
        "    union_area = boxes1_area + boxes2_area - inter_area\n",
        "    iou = inter_area / (union_area + K.epsilon())\n",
        "\n",
        "    # The coordinates of the upper left corner and the lower right corner of the enclosing rectangle, the shape is (8, 13, 13, 3, 2)\n",
        "    enclose_left_up = tf.minimum(boxes1_x0y0x1y1[..., :2], boxes2_x0y0x1y1[..., :2])\n",
        "    enclose_right_down = tf.maximum(boxes1_x0y0x1y1[..., 2:], boxes2_x0y0x1y1[..., 2:])\n",
        "\n",
        "    # DIoU 계산\n",
        "\n",
        "    # The square of the diagonal of the enclosing rectangle\n",
        "    enclose_wh = enclose_right_down - enclose_left_up\n",
        "    enclose_c2 = K.pow(enclose_wh[..., 0], 2) + K.pow(enclose_wh[..., 1], 2)\n",
        "    \n",
        "    # The square of the distance between the center points of the two rectangles\n",
        "    p2 = K.pow(boxes1[..., 0] - boxes2[..., 0], 2) + K.pow(boxes1[..., 1] - boxes2[..., 1], 2)\n",
        "\n",
        "    # Increase av. Add division by 0 protection to prevent nan.\n",
        "    atan1 = tf.atan(boxes1[..., 2] / (boxes1[..., 3] + K.epsilon()))\n",
        "    atan2 = tf.atan(boxes2[..., 2] / (boxes2[..., 3] + K.epsilon()))\n",
        "    v = 4.0 * K.pow(atan1 - atan2, 2) / (math.pi ** 2)\n",
        "    a = v / (1 - iou + v)\n",
        "\n",
        "    # ciou loss = 1 - ciou\n",
        "    ciou = iou - 1.0 * p2 / enclose_c2 - 1.0 * a * v\n",
        "    return ciou\n",
        "\n",
        "def bbox_iou(boxes1, boxes2):\n",
        "    '''\n",
        "    Measuring frame boxes1 (?, grid_h, grid_w, 3, 1, 4), the output of the neural network (tx, ty, tw, th) is obtained by post-processing (bx, by, bw, bh)\n",
        "    All gt boxes2 in the picture (?, 1, 1, 1, 150, 4)\n",
        "    '''\n",
        "    boxes1_area = boxes1[..., 2] * boxes1[..., 3]  # Area of 3 prediction boxes of all grids\n",
        "    boxes2_area = boxes2[..., 2] * boxes2[..., 3]  # The area of all ground truth\n",
        "\n",
        "    # (x, y, w, h) to (x0, y0, x1, y1)\n",
        "    boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n",
        "                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n",
        "    boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n",
        "                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n",
        "\n",
        "    # 3 prediction boxes with grids and 150 ground truth calculation iou respectively. So the shape of left_up and right_down = (?, grid_h, grid_w, 3, 150, 2)\n",
        "    left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])  # The coordinates of the upper left corner of the intersecting rectangle\n",
        "    right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])  # The coordinates of the lower right corner of the intersecting rectangle\n",
        "\n",
        "    inter_section = tf.maximum(right_down - left_up, 0.0)  # The w and h of the intersecting rectangle are 0 when they are negative (?, grid_h, grid_w, 3, 150, 2)\n",
        "    inter_area = inter_section[..., 0] * inter_section[..., 1]  # Area of the intersecting rectangle (?, grid_h, grid_w, 3, 150)\n",
        "    union_area = boxes1_area + boxes2_area - inter_area  # union_area      (?, grid_h, grid_w, 3, 150)\n",
        "    iou = 1.0 * inter_area / union_area  # iou                             (?, grid_h, grid_w, 3, 150)\n",
        "    return iou\n",
        "\n",
        "def loss_layer(conv, pred, label, bboxes, stride, num_class, iou_loss_thresh):\n",
        "    conv_shape = tf.shape(conv)\n",
        "    batch_size = conv_shape[0]\n",
        "    output_size = conv_shape[1]\n",
        "    input_size = stride * output_size\n",
        "    conv = tf.reshape(conv, (batch_size, output_size, output_size,\n",
        "                             3, 5 + num_class))\n",
        "    conv_raw_prob = conv[:, :, :, :, 5:]\n",
        "\n",
        "    pred_xywh = pred[:, :, :, :, 0:4]\n",
        "    pred_conf = pred[:, :, :, :, 4:5]\n",
        "\n",
        "    label_xywh = label[:, :, :, :, 0:4]\n",
        "    respond_bbox = label[:, :, :, :, 4:5]\n",
        "    label_prob = label[:, :, :, :, 5:]\n",
        "\n",
        "    ciou = tf.expand_dims(bbox_ciou(pred_xywh, label_xywh), axis=-1)  # (8, 13, 13, 3, 1)\n",
        "    input_size = tf.cast(input_size, tf.float32)\n",
        "\n",
        "    # The weight of each prediction box xxxiou_loss = 2-(ground truth area/picture area)\n",
        "    bbox_loss_scale = 2.0 - 1.0 * label_xywh[:, :, :, :, 2:3] * label_xywh[:, :, :, :, 3:4] / (input_size ** 2)\n",
        "    ciou_loss = respond_bbox * bbox_loss_scale * (1 - ciou) # 1. respond_bbox is used as a mask, xxxiou_loss is calculated only when there is an object\n",
        "\n",
        "    # 2. respond_bbox is used as a mask, and the category loss is calculated when there is an object\n",
        "    prob_loss = respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=label_prob, logits=conv_raw_prob)\n",
        "\n",
        "    # 3. xxxiou_loss and category loss are relatively simple. The important thing is conf_loss, which is a focal_loss\n",
        "    # There are two steps: the first step is to determine which grid_h * grid_w * 3 prediction boxes are used as counterexamples; the second step is to calculate focal_loss.\n",
        "\n",
        "    expand_pred_xywh = pred_xywh[:, :, :, :, np.newaxis, :]  # Expand to (?, grid_h, grid_w, 3,   1, 4)\n",
        "    expand_bboxes = bboxes[:, np.newaxis, np.newaxis, np.newaxis, :, :]  # Expand to (?,      1,      1, 1, 150, 4)\n",
        "    iou = bbox_iou(expand_pred_xywh, expand_bboxes)  # The 3 prediction boxes of all grids and 150 ground truth respectively calculate iou.   (?, grid_h, grid_w, 3, 150\n",
        "    max_iou = tf.expand_dims(tf.reduce_max(iou, axis=-1), axis=-1)  # Among 150 ground truth iou, keep the largest iou. (?, grid_h, grid_w, 3, 1)\n",
        "\n",
        "    # respond_bgd represents whether the grid_h * grid_w * 3 prediction boxes output by this branch are counterexamples (background)\n",
        "    # label has an object, respond_bgd is 0. If there is no object: if the iou with a certain gt (150 in total) exceeds iou_loss_thresh, respond_bgd is 0; if the iou with all gt (150 at most) is less than iou_loss_thresh, respond_bgd is 1.\n",
        "    # respond_bgd is 0 means there is an object, which is not a counterexample; the weight respond_bgd is 1 means there is no object, which is a counterexample.\n",
        "    # Interestingly, due to constant updates during model training, for the same picture, the grid_h * grid_w * 3 prediction boxes (for this branch output) of the two predictions are different. These prediction boxes are used to calculate iou to determine which prediction boxes are counterexamples.\n",
        "    # Instead of using a priori box of fixed size (not fixed position).\n",
        "    respond_bgd = (1.0 - respond_bbox) * tf.cast(max_iou < iou_loss_thresh, tf.float32)\n",
        "\n",
        "    # Binary cross entropy loss\n",
        "    pos_loss = respond_bbox * (0 - K.log(pred_conf + K.epsilon()))\n",
        "    neg_loss = respond_bgd  * (0 - K.log(1 - pred_conf + K.epsilon()))\n",
        "\n",
        "    conf_loss = pos_loss + neg_loss\n",
        "    # Looking back at respond_bgd, the iou of a certain prediction box and a certain gt exceeds iou_loss_thresh, which is not regarded as a counterexample. When participating in the \"Binary Cross Entropy of Predicted Confidence Level and True Confidence Level\", this box may not be a positive example (if this box is not marked as 1 in the label). This box may not participate in the calculation of confidence loss.\n",
        "    # This kind of box is generally the box near the gt box, or the other two boxes of the grid where the gt box is located. It is neither a positive example nor a negative example. It does not participate in the calculation of the confidence loss. (Called ignore in the paper\n",
        "\n",
        "    ciou_loss = tf.reduce_mean(tf.reduce_sum(ciou_loss, axis=[1, 2, 3, 4]))  # Each sample calculates its own ciou_loss separately, and then averages\n",
        "    conf_loss = tf.reduce_mean(tf.reduce_sum(conf_loss, axis=[1, 2, 3, 4]))  # Each sample calculates its own conf_loss separately, and then averages\n",
        "    prob_loss = tf.reduce_mean(tf.reduce_sum(prob_loss, axis=[1, 2, 3, 4]))  # Each sample calculates its own prob_loss separately, and then averages\n",
        "\n",
        "    return ciou_loss, conf_loss, prob_loss\n",
        "\n",
        "# YOLO의 모델 출력 결과를 scale및 grid에 따라 좌표 복원하기\n",
        "# conf(objectness)와 prob(class probability)에는 sigmoid activation function 적용\n",
        "# 전체가 convolution이라서 부분적으로 activation을 후처리함.\n",
        "\n",
        "def decode(conv_output, anchors, stride, num_class):\n",
        "    conv_shape       = tf.shape(conv_output)\n",
        "    batch_size       = conv_shape[0]\n",
        "    output_size      = conv_shape[1]\n",
        "    anchor_per_scale = len(anchors)\n",
        "    conv_output = tf.reshape(conv_output, (batch_size, output_size, output_size, anchor_per_scale, 5 + num_class))\n",
        "    conv_raw_dxdy = conv_output[:, :, :, :, 0:2]\n",
        "    conv_raw_dwdh = conv_output[:, :, :, :, 2:4]\n",
        "    conv_raw_conf = conv_output[:, :, :, :, 4:5]\n",
        "    conv_raw_prob = conv_output[:, :, :, :, 5: ]\n",
        "    y = tf.tile(tf.range(output_size, dtype=tf.int32)[:, tf.newaxis], [1, output_size])\n",
        "    x = tf.tile(tf.range(output_size, dtype=tf.int32)[tf.newaxis, :], [output_size, 1])\n",
        "    xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-1)\n",
        "    xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :], [batch_size, 1, 1, anchor_per_scale, 1])\n",
        "    xy_grid = tf.cast(xy_grid, tf.float32)\n",
        "    pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * stride\n",
        "    pred_wh = (tf.exp(conv_raw_dwdh) * anchors) * stride\n",
        "    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)\n",
        "    pred_conf = tf.sigmoid(conv_raw_conf)\n",
        "    pred_prob = tf.sigmoid(conv_raw_prob)\n",
        "    return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)\n",
        "\n",
        "# 전체 loss 계산 함수\n",
        "\n",
        "def yolo_loss(args, num_classes, iou_loss_thresh, anchors):\n",
        "    conv_lbbox = args[0]   # (?, ?, ?, 3*(num_classes+5))\n",
        "    conv_mbbox = args[1]   # (?, ?, ?, 3*(num_classes+5))\n",
        "    conv_sbbox = args[2]   # (?, ?, ?, 3*(num_classes+5))\n",
        "    label_sbbox = args[3]   # (?, ?, ?, 3, num_classes+5)\n",
        "    label_mbbox = args[4]   # (?, ?, ?, 3, num_classes+5)\n",
        "    label_lbbox = args[5]   # (?, ?, ?, 3, num_classes+5)\n",
        "    true_sbboxes = args[6]   # (?, 150, 4)\n",
        "    true_mbboxes = args[7]   # (?, 150, 4)\n",
        "    true_lbboxes = args[8]   # (?, 150, 4)\n",
        "    pred_sbbox = decode(conv_sbbox, anchors[0], 8, num_classes)\n",
        "    pred_mbbox = decode(conv_mbbox, anchors[1], 16, num_classes)\n",
        "    pred_lbbox = decode(conv_lbbox, anchors[2], 32, num_classes)\n",
        "    sbbox_ciou_loss, sbbox_conf_loss, sbbox_prob_loss = loss_layer(conv_sbbox, pred_sbbox, label_sbbox, true_sbboxes, 8, num_classes, iou_loss_thresh)\n",
        "    mbbox_ciou_loss, mbbox_conf_loss, mbbox_prob_loss = loss_layer(conv_mbbox, pred_mbbox, label_mbbox, true_mbboxes, 16, num_classes, iou_loss_thresh)\n",
        "    lbbox_ciou_loss, lbbox_conf_loss, lbbox_prob_loss = loss_layer(conv_lbbox, pred_lbbox, label_lbbox, true_lbboxes, 32, num_classes, iou_loss_thresh)\n",
        "\n",
        "    ciou_loss = sbbox_ciou_loss + mbbox_ciou_loss + lbbox_ciou_loss\n",
        "    conf_loss = sbbox_conf_loss + mbbox_conf_loss + lbbox_conf_loss\n",
        "    prob_loss = sbbox_prob_loss + mbbox_prob_loss + lbbox_prob_loss\n",
        "\n",
        "    loss = ciou_loss + conf_loss + prob_loss\n",
        "\n",
        "    loss = tf.compat.v1.Print(loss, [loss, ciou_loss, conf_loss, prob_loss], message=' loss: ')\n",
        "    #tf.print([loss, ciou_loss, conf_loss, prob_loss])\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjphI73fVSYt"
      },
      "source": [
        "**YOLO 전체 모델 생성**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfKE1HvGVWj7"
      },
      "source": [
        "def create_model(input_shape, anchors_stride_base, num_classes, load_pretrained=True, freeze_body=2,\n",
        "            weights_path='model_data/yolo_weights.h5'):\n",
        "    '''create the training model'''\n",
        "    K.clear_session() # get a new session\n",
        "    image_input = Input(shape=(None, None, 3))\n",
        "    h, w = input_shape\n",
        "    num_anchors = len(anchors_stride_base)\n",
        "\n",
        "    max_bbox_per_scale = 150\n",
        "    iou_loss_thresh = 0.7\n",
        "\n",
        "    model_body = yolo4_body(image_input, num_anchors, num_classes)\n",
        "    print('Create YOLOv4 model with {} anchors and {} classes.'.format(num_anchors*3, num_classes))\n",
        "\n",
        "    if load_pretrained:\n",
        "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
        "        print('Load weights {}.'.format(weights_path))\n",
        "        if freeze_body in [1, 2]:\n",
        "            # Freeze darknet53 body or freeze all but 3 output layers.\n",
        "            num = (250, len(model_body.layers)-3)[freeze_body-1]\n",
        "            for i in range(num): model_body.layers[i].trainable = False\n",
        "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
        "\n",
        "    y_true = [\n",
        "        layers.Input(name='input_2', shape=(None, None, 3, (num_classes + 5))),  # label_sbbox\n",
        "        layers.Input(name='input_3', shape=(None, None, 3, (num_classes + 5))),  # label_mbbox\n",
        "        layers.Input(name='input_4', shape=(None, None, 3, (num_classes + 5))),  # label_lbbox\n",
        "        layers.Input(name='input_5', shape=(max_bbox_per_scale, 4)),             # true_sbboxes\n",
        "        layers.Input(name='input_6', shape=(max_bbox_per_scale, 4)),             # true_mbboxes\n",
        "        layers.Input(name='input_7', shape=(max_bbox_per_scale, 4))              # true_lbboxes\n",
        "    ]\n",
        "    loss_list = layers.Lambda(yolo_loss, name='yolo_loss',\n",
        "                           arguments={'num_classes': num_classes, 'iou_loss_thresh': iou_loss_thresh,\n",
        "                                      'anchors': anchors_stride_base})([*model_body.output, *y_true])\n",
        "\n",
        "    model = Model([model_body.input, *y_true], loss_list)\n",
        "    #model.summary()\n",
        "\n",
        "    return model, model_body"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYdqMlWsVhM7"
      },
      "source": [
        "**class와 anchor 설정파일 읽어오기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCZYGwAiybVG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3aab01c-e993-4b80-ea09-5acc6a0ba55f"
      },
      "source": [
        "!cat model_data/voc_classes.txt\n",
        "!cat model_data/yolo4_anchors.txt"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aeroplane\n",
            "bicycle\n",
            "bird\n",
            "boat\n",
            "bottle\n",
            "bus\n",
            "car\n",
            "cat\n",
            "chair\n",
            "cow\n",
            "diningtable\n",
            "dog\n",
            "horse\n",
            "motorbike\n",
            "person\n",
            "pottedplant\n",
            "sheep\n",
            "sofa\n",
            "train\n",
            "tvmonitor\n",
            "12, 16,  19, 36,  40, 28,  36, 75,  76, 55,  72, 146,  142, 110,  192, 243,  459, 401"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ALGqjKJVkNC"
      },
      "source": [
        "def get_classes(classes_path):\n",
        "    '''loads the classes'''\n",
        "    with open(classes_path) as f:\n",
        "        class_names = f.readlines()\n",
        "    class_names = [c.strip() for c in class_names]\n",
        "    return class_names\n",
        "\n",
        "def get_anchors(anchors_path):\n",
        "    '''loads the anchors from a file'''\n",
        "    with open(anchors_path) as f:\n",
        "        anchors = f.readline()\n",
        "    anchors = [float(x) for x in anchors.split(',')]\n",
        "    return np.array(anchors).reshape(-1, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBq5i6WCWJE7"
      },
      "source": [
        "**Data augmentation modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrC07F6UWMIE"
      },
      "source": [
        "def random_fill(image, bboxes):\n",
        "    if random.random() < 0.5:\n",
        "        h, w, _ = image.shape\n",
        "        # Fill the black border horizontally to train small target detection\n",
        "        if random.random() < 0.5:\n",
        "            dx = random.randint(int(0.5*w), int(1.5*w))\n",
        "            black_1 = np.zeros((h, dx, 3), dtype='uint8')\n",
        "            black_2 = np.zeros((h, dx, 3), dtype='uint8')\n",
        "            image = np.concatenate([black_1, image, black_2], axis=1)\n",
        "            bboxes[:, [0, 2]] += dx\n",
        "        # Fill the black edges vertically to train small target detection\n",
        "        else:\n",
        "            dy = random.randint(int(0.5*h), int(1.5*h))\n",
        "            black_1 = np.zeros((dy, w, 3), dtype='uint8')\n",
        "            black_2 = np.zeros((dy, w, 3), dtype='uint8')\n",
        "            image = np.concatenate([black_1, image, black_2], axis=0)\n",
        "            bboxes[:, [1, 3]] += dy\n",
        "    return image, bboxes\n",
        "\n",
        "def random_horizontal_flip(image, bboxes):\n",
        "    if random.random() < 0.5:\n",
        "        _, w, _ = image.shape\n",
        "        image = image[:, ::-1, :]\n",
        "        bboxes[:, [0,2]] = w - bboxes[:, [2,0]]\n",
        "    return image, bboxes\n",
        "\n",
        "def random_crop(image, bboxes):\n",
        "    if random.random() < 0.5:\n",
        "        h, w, _ = image.shape\n",
        "        max_bbox = np.concatenate([np.min(bboxes[:, 0:2], axis=0), np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n",
        "\n",
        "        max_l_trans = max_bbox[0]\n",
        "        max_u_trans = max_bbox[1]\n",
        "        max_r_trans = w - max_bbox[2]\n",
        "        max_d_trans = h - max_bbox[3]\n",
        "\n",
        "        crop_xmin = max(0, int(max_bbox[0] - random.uniform(0, max_l_trans)))\n",
        "        crop_ymin = max(0, int(max_bbox[1] - random.uniform(0, max_u_trans)))\n",
        "        crop_xmax = max(w, int(max_bbox[2] + random.uniform(0, max_r_trans)))\n",
        "        crop_ymax = max(h, int(max_bbox[3] + random.uniform(0, max_d_trans)))\n",
        "\n",
        "        image = image[crop_ymin : crop_ymax, crop_xmin : crop_xmax]\n",
        "\n",
        "        bboxes[:, [0, 2]] = bboxes[:, [0, 2]] - crop_xmin\n",
        "        bboxes[:, [1, 3]] = bboxes[:, [1, 3]] - crop_ymin\n",
        "    return image, bboxes\n",
        "\n",
        "def random_translate(image, bboxes):\n",
        "    if random.random() < 0.5:\n",
        "        h, w, _ = image.shape\n",
        "        max_bbox = np.concatenate([np.min(bboxes[:, 0:2], axis=0), np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n",
        "\n",
        "        max_l_trans = max_bbox[0]\n",
        "        max_u_trans = max_bbox[1]\n",
        "        max_r_trans = w - max_bbox[2]\n",
        "        max_d_trans = h - max_bbox[3]\n",
        "\n",
        "        tx = random.uniform(-(max_l_trans - 1), (max_r_trans - 1))\n",
        "        ty = random.uniform(-(max_u_trans - 1), (max_d_trans - 1))\n",
        "\n",
        "        M = np.array([[1, 0, tx], [0, 1, ty]])\n",
        "        image = cv2.warpAffine(image, M, (w, h))\n",
        "\n",
        "        bboxes[:, [0, 2]] = bboxes[:, [0, 2]] + tx\n",
        "        bboxes[:, [1, 3]] = bboxes[:, [1, 3]] + ty\n",
        "    return image, bboxes\n",
        "\n",
        "def image_preprocess(image, target_size, gt_boxes):\n",
        "    # The images passed in for training are in rgb format\n",
        "    ih, iw = target_size\n",
        "    h, w = image.shape[:2]\n",
        "    interps = [   # Randomly choose an interpolation method\n",
        "        cv2.INTER_NEAREST,\n",
        "        cv2.INTER_LINEAR,\n",
        "        cv2.INTER_AREA,\n",
        "        cv2.INTER_CUBIC,\n",
        "        cv2.INTER_LANCZOS4,\n",
        "    ]\n",
        "    method = np.random.choice(interps)   # Randomly choose an interpolation method\n",
        "    scale_x = float(iw) / w\n",
        "    scale_y = float(ih) / h\n",
        "    image = cv2.resize(image, None, None, fx=scale_x, fy=scale_y, interpolation=method)\n",
        "\n",
        "    pimage = image.astype(np.float32) / 255.\n",
        "    if gt_boxes is None:\n",
        "        return pimage\n",
        "    else:\n",
        "        gt_boxes[:, [0, 2]] = gt_boxes[:, [0, 2]] * scale_x\n",
        "        gt_boxes[:, [1, 3]] = gt_boxes[:, [1, 3]] * scale_y\n",
        "        return pimage, gt_boxes\n",
        "\n",
        "def parse_annotation(annotation, train_input_size, annotation_type):\n",
        "    line = annotation.split()\n",
        "    image_path = line[0]\n",
        "    if not os.path.exists(image_path):\n",
        "        raise KeyError(\"%s does not exist ... \" %image_path)\n",
        "    image = np.array(cv2.imread(image_path))\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # No items are marked, that is, each grid is treated as a background\n",
        "    exist_boxes = True\n",
        "    if len(line) == 1:\n",
        "        bboxes = np.array([[10, 10, 101, 103, 0]])\n",
        "        exist_boxes = False\n",
        "    else:\n",
        "        bboxes = np.array([list(map(lambda x: int(float(x)), box.split(','))) for box in line[1:]])\n",
        "    if annotation_type == 'train':\n",
        "        # image, bboxes = random_fill(np.copy(image), np.copy(bboxes))    # Open when the dataset lacks small objects\n",
        "        image, bboxes = random_horizontal_flip(np.copy(image), np.copy(bboxes))\n",
        "        image, bboxes = random_crop(np.copy(image), np.copy(bboxes))\n",
        "        image, bboxes = random_translate(np.copy(image), np.copy(bboxes))\n",
        "    image, bboxes = image_preprocess(np.copy(image), [train_input_size, train_input_size], np.copy(bboxes))\n",
        "    return image, bboxes, exist_boxes"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvXettUHWdPW"
      },
      "source": [
        "**Data generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fo-OE7DyWgOS"
      },
      "source": [
        "def data_generator(annotation_lines, batch_size, anchors, num_classes, max_bbox_per_scale, annotation_type):\n",
        "    '''data generator for fit_generator'''\n",
        "    n = len(annotation_lines)\n",
        "    i = 0\n",
        "    # Multi-scale training\n",
        "    train_input_sizes = [320, 352, 384, 416, 448, 480, 512, 544, 576, 608]\n",
        "    strides = np.array([8, 16, 32])\n",
        "\n",
        "    while True:\n",
        "        train_input_size = random.choice(train_input_sizes)\n",
        "\n",
        "        # Number of output grids\n",
        "        train_output_sizes = train_input_size // strides\n",
        "\n",
        "        batch_image = np.zeros((batch_size, train_input_size, train_input_size, 3))\n",
        "\n",
        "        batch_label_sbbox = np.zeros((batch_size, train_output_sizes[0], train_output_sizes[0],\n",
        "                                      3, 5 + num_classes))\n",
        "        batch_label_mbbox = np.zeros((batch_size, train_output_sizes[1], train_output_sizes[1],\n",
        "                                      3, 5 + num_classes))\n",
        "        batch_label_lbbox = np.zeros((batch_size, train_output_sizes[2], train_output_sizes[2],\n",
        "                                      3, 5 + num_classes))\n",
        "\n",
        "        batch_sbboxes = np.zeros((batch_size, max_bbox_per_scale, 4))\n",
        "        batch_mbboxes = np.zeros((batch_size, max_bbox_per_scale, 4))\n",
        "        batch_lbboxes = np.zeros((batch_size, max_bbox_per_scale, 4))\n",
        "\n",
        "        for num in range(batch_size):\n",
        "            if i == 0:\n",
        "                np.random.shuffle(annotation_lines)\n",
        "\n",
        "            image, bboxes, exist_boxes = parse_annotation(annotation_lines[i], train_input_size, annotation_type)\n",
        "            # 각각의 앵커에 맞는 사전 작업\n",
        "            label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes = preprocess_true_boxes(bboxes, train_output_sizes, strides, num_classes, max_bbox_per_scale, anchors)\n",
        "\n",
        "            batch_image[num, :, :, :] = image\n",
        "            if exist_boxes:\n",
        "                batch_label_sbbox[num, :, :, :, :] = label_sbbox\n",
        "                batch_label_mbbox[num, :, :, :, :] = label_mbbox\n",
        "                batch_label_lbbox[num, :, :, :, :] = label_lbbox\n",
        "                batch_sbboxes[num, :, :] = sbboxes\n",
        "                batch_mbboxes[num, :, :] = mbboxes\n",
        "                batch_lbboxes[num, :, :] = lbboxes\n",
        "            i = (i + 1) % n\n",
        "        yield [batch_image, batch_label_sbbox, batch_label_mbbox, batch_label_lbbox, batch_sbboxes, batch_mbboxes, batch_lbboxes], np.zeros(batch_size)\n",
        "\n",
        "def data_generator_wrapper(annotation_lines, batch_size, anchors, num_classes, max_bbox_per_scale, annotation_type):\n",
        "    n = len(annotation_lines)\n",
        "    if n==0 or batch_size<=0: return None\n",
        "    return data_generator(annotation_lines, batch_size, anchors, num_classes, max_bbox_per_scale, annotation_type)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAmUczF9ZzBz"
      },
      "source": [
        "**Ground Truth box 처리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CNvYGDeZ3Q1"
      },
      "source": [
        "# IOU 계산\n",
        "def bbox_iou_data(boxes1, boxes2):\n",
        "    boxes1 = np.array(boxes1)\n",
        "    boxes2 = np.array(boxes2)\n",
        "    boxes1_area = boxes1[..., 2] * boxes1[..., 3]\n",
        "    boxes2_area = boxes2[..., 2] * boxes2[..., 3]\n",
        "    # x1,y1,x2,y2 ==> x,y,w,h\n",
        "    boxes1 = np.concatenate([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n",
        "                            boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n",
        "    boxes2 = np.concatenate([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n",
        "                            boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n",
        "    # 교집합의 좌상\n",
        "    left_up = np.maximum(boxes1[..., :2], boxes2[..., :2])\n",
        "    # 교집합의 우하\n",
        "    right_down = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n",
        "    inter_section = np.maximum(right_down - left_up, 0.0)\n",
        "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
        "    union_area = boxes1_area + boxes2_area - inter_area\n",
        "    return inter_area / union_area\n",
        "\n",
        "# GT boxes를 anchor로 scale을 찾고 해당 scale의 grid와 anchor에 저장\n",
        "def preprocess_true_boxes(bboxes, train_output_sizes, strides, num_classes, max_bbox_per_scale, anchors):\n",
        "    label = [np.zeros((train_output_sizes[i], train_output_sizes[i], 3,\n",
        "                       5 + num_classes)) for i in range(3)]\n",
        "    bboxes_xywh = [np.zeros((max_bbox_per_scale, 4)) for _ in range(3)]\n",
        "    bbox_count = np.zeros((3,))\n",
        "    for bbox in bboxes:\n",
        "        bbox_coor = bbox[:4]\n",
        "        bbox_class_ind = bbox[4]\n",
        "        onehot = np.zeros(num_classes, dtype=np.float)\n",
        "        onehot[bbox_class_ind] = 1.0\n",
        "        bbox_xywh = np.concatenate([(bbox_coor[2:] + bbox_coor[:2]) * 0.5, bbox_coor[2:] - bbox_coor[:2]], axis=-1)\n",
        "        bbox_xywh_scaled = 1.0 * bbox_xywh[np.newaxis, :] / strides[:, np.newaxis]\n",
        "        # 3단계 크기 중에서 anchor와 가장 매칭되는 곳 찾기\n",
        "        iou = []\n",
        "        for i in range(3):\n",
        "            anchors_xywh = np.zeros((3, 4))\n",
        "            anchors_xywh[:, 0:2] = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5\n",
        "            anchors_xywh[:, 2:4] = anchors[i]\n",
        "            iou_scale = bbox_iou_data(bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh)\n",
        "            iou.append(iou_scale)\n",
        "        best_anchor_ind = np.argmax(np.array(iou).reshape(-1), axis=-1)\n",
        "        best_detect = int(best_anchor_ind / 3)\n",
        "        best_anchor = int(best_anchor_ind % 3)\n",
        "        xind, yind = np.floor(bbox_xywh_scaled[best_detect, 0:2]).astype(np.int32)\n",
        "        # Prevent crossing\n",
        "        grid_r = label[best_detect].shape[0]\n",
        "        grid_c = label[best_detect].shape[1]\n",
        "        xind = max(0, xind)\n",
        "        yind = max(0, yind)\n",
        "        xind = min(xind, grid_r-1)\n",
        "        yind = min(yind, grid_c-1)\n",
        "        # bbox[4], objectness, class GT\n",
        "        label[best_detect][yind, xind, best_anchor, :] = 0\n",
        "        label[best_detect][yind, xind, best_anchor, 0:4] = bbox_xywh\n",
        "        label[best_detect][yind, xind, best_anchor, 4:5] = 1.0\n",
        "        label[best_detect][yind, xind, best_anchor, 5:] = onehot\n",
        "        bbox_ind = int(bbox_count[best_detect] % max_bbox_per_scale)\n",
        "        bboxes_xywh[best_detect][bbox_ind, :4] = bbox_xywh\n",
        "        bbox_count[best_detect] += 1\n",
        "    label_sbbox, label_mbbox, label_lbbox = label\n",
        "    sbboxes, mbboxes, lbboxes = bboxes_xywh\n",
        "    return label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJokqOZgVs56"
      },
      "source": [
        "**학습하기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNs1Bmugazdv"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "from tensorflow.keras.layers import Input, Lambda\n",
        "import tensorflow.keras.layers as layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "from callback_eval import Evaluate"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JzqobiwVueu"
      },
      "source": [
        "def _main():\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "      try:\n",
        "        for gpu in gpus:\n",
        "          tf.config.experimental.set_memory_growth(gpu, True)\n",
        "      except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "    print('Please visit https://github.com/miemie2013/Keras-YOLOv4 for more complete model!')\n",
        "\n",
        "    annotation_train_path = '2012_train.txt'\n",
        "    annotation_val_path = '2012_val.txt'\n",
        "    #annotation_train_path = '2007_train.txt'\n",
        "    #annotation_val_path = '2007_val.txt'\n",
        "    log_dir = 'logs/000/'\n",
        "    classes_path = 'model_data/voc_classes.txt'\n",
        "    anchors_path = 'model_data/yolo4_anchors.txt'\n",
        "    class_names = get_classes(classes_path)\n",
        "    num_classes = len(class_names)\n",
        "    class_index = ['{}'.format(i) for i in range(num_classes)]\n",
        "    anchors = get_anchors(anchors_path)\n",
        "\n",
        "    max_bbox_per_scale = 150\n",
        "\n",
        "    anchors_stride_base = np.array([\n",
        "        [[12, 16], [19, 36], [40, 28]],\n",
        "        [[36, 75], [76, 55], [72, 146]],\n",
        "        [[142, 110], [192, 243], [459, 401]]\n",
        "    ])\n",
        "    # Some preprocessing\n",
        "    anchors_stride_base = anchors_stride_base.astype(np.float32)\n",
        "    anchors_stride_base[0] /= 8\n",
        "    anchors_stride_base[1] /= 16\n",
        "    anchors_stride_base[2] /= 32\n",
        "\n",
        "    input_shape = (608, 608) # multiple of 32, hw\n",
        "    model_path = 'yolo4_weight.h5'\n",
        "    #model_path = 'logs/000/'+'ep001-loss5261.659.h5'\n",
        "    #model_path = 'logs-neck/000/'+'ep050-loss5.966.h5' # neck for voc2012\n",
        "    model, model_body = create_model(input_shape, anchors_stride_base, num_classes,\n",
        "      load_pretrained=True, freeze_body=2,\n",
        "      weights_path=model_path)\n",
        "\n",
        "    logging = TensorBoard(log_dir=log_dir)\n",
        "    checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}.h5',\n",
        "        monitor='loss', save_weights_only=True, save_best_only=True, period=1)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=3, verbose=1)\n",
        "    early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=10, verbose=1)\n",
        "    evaluation = Evaluate(model_body=model_body, anchors=anchors, class_names=class_index, score_threshold=0.05,\n",
        "        tensorboard=logging, weighted_average=True, eval_file=annotation_val_path, log_dir=log_dir)\n",
        "\n",
        "    with open(annotation_train_path) as f:\n",
        "        lines_train = f.readlines()\n",
        "    np.random.seed(10101)\n",
        "    np.random.shuffle(lines_train)\n",
        "    np.random.seed(None)\n",
        "    num_train = len(lines_train)\n",
        "\n",
        "    with open(annotation_val_path) as f:\n",
        "        lines_val = f.readlines()\n",
        "    np.random.seed(10101)\n",
        "    np.random.shuffle(lines_val)\n",
        "    np.random.seed(None)\n",
        "    num_val = len(lines_val)\n",
        "    # Train with frozen layers first, to get a stable loss.\n",
        "    # Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n",
        "    if True:\n",
        "        model.compile(optimizer=Adam(lr=1e-3), loss={'yolo_loss': lambda y_true, y_pred: y_pred})\n",
        "\n",
        "        batch_size = 16\n",
        "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
        "        #model.fit_generator(data_generator_wrapper(lines_train[:num_train], batch_size, input_shape, anchors, num_classes),\n",
        "        #model.fit_generator(data_generator_wrapper(lines_train, batch_size, anchors_stride_base, num_classes, max_bbox_per_scale, 'train'),\n",
        "        model.fit(data_generator_wrapper(lines_train, batch_size, anchors_stride_base, num_classes, max_bbox_per_scale, 'train'),\n",
        "                steps_per_epoch=max(1, num_train//batch_size),\n",
        "                epochs=50,\n",
        "                initial_epoch=0,\n",
        "                callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n",
        "\n",
        "    # Unfreeze and continue training, to fine-tune.\n",
        "    # Train longer if the result is not good.\n",
        "    if True:\n",
        "        for i in range(len(model.layers)):\n",
        "            model.layers[i].trainable = True\n",
        "        model.compile(optimizer=Adam(lr=1e-5), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
        "        print('Unfreeze all of the layers.')\n",
        "\n",
        "        batch_size = 4 # note that more GPU memory is required after unfreezing the body\n",
        "        #batch_size = 2 # note that more GPU memory is required after unfreezing the body\n",
        "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
        "        model.fit_generator(data_generator_wrapper(lines_train, batch_size, anchors_stride_base, num_classes, max_bbox_per_scale, 'train'),\n",
        "            steps_per_epoch=max(1, num_train//batch_size),\n",
        "            epochs=50000,\n",
        "            initial_epoch=50,\n",
        "            callbacks=[logging, checkpoint, reduce_lr, early_stopping, evaluation])\n",
        "\n",
        "    # Further training if needed."
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAhenvNDr9vQ"
      },
      "source": [
        "!cp /mydrive/yolo4_weight.h5 ."
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0AFas_2bUp8"
      },
      "source": [
        "**주의: 아래 YOLO의 학습에는 많은\u0003시간이 소요됩니다.**  \n",
        "Google colab 무료 버전의 한계 때문에 colab에서 끝까지 학습하는 것은 어렵습니다.  \n",
        "학습이 시작되고 정상적으로 진행되는 데 까지만 확인하도록 합시다.  \n",
        "완전한 학습을 위해서는 GPU를 갖춘 local desktop 개발 환경에서 수행할 필요가 있습니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o53sHgeYaicF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e2e1adc-6627-4df5-c710-6f52803e1222"
      },
      "source": [
        "_main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Physical devices cannot be modified after being initialized\n",
            "Please visit https://github.com/miemie2013/Keras-YOLOv4 for more complete model!\n",
            "Create YOLOv4 model with 9 anchors and 20 classes.\n",
            "WARNING:tensorflow:Skipping loading of weights for layer conv2d_109 due to mismatch in shape ((1, 1, 1024, 75) vs (255, 1024, 1, 1)).\n",
            "WARNING:tensorflow:Skipping loading of weights for layer conv2d_109 due to mismatch in shape ((75,) vs (255,)).\n",
            "WARNING:tensorflow:Skipping loading of weights for layer conv2d_101 due to mismatch in shape ((1, 1, 512, 75) vs (255, 512, 1, 1)).\n",
            "WARNING:tensorflow:Skipping loading of weights for layer conv2d_101 due to mismatch in shape ((75,) vs (255,)).\n",
            "WARNING:tensorflow:Skipping loading of weights for layer conv2d_93 due to mismatch in shape ((1, 1, 256, 75) vs (255, 256, 1, 1)).\n",
            "WARNING:tensorflow:Skipping loading of weights for layer conv2d_93 due to mismatch in shape ((75,) vs (255,)).\n",
            "Load weights yolo4_weight.h5.\n",
            "Freeze the first 367 layers of total 370 layers.\n",
            "WARNING:tensorflow:From <ipython-input-17-c13c1f63c555>:196: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.\n",
            "Instructions for updating:\n",
            "Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\n",
            "\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Train on 5717 samples, val on 5823 samples, with batch size 16.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "357/357 [==============================] - 443s 1s/step - loss: 781.0318\n",
            "Epoch 2/50\n",
            "357/357 [==============================] - 342s 959ms/step - loss: 86.1122\n",
            "Epoch 3/50\n",
            "357/357 [==============================] - 341s 958ms/step - loss: 45.7343\n",
            "Epoch 4/50\n",
            "357/357 [==============================] - 336s 944ms/step - loss: 30.9916\n",
            "Epoch 5/50\n",
            "357/357 [==============================] - 341s 955ms/step - loss: 23.7067\n",
            "Epoch 6/50\n",
            "357/357 [==============================] - 338s 948ms/step - loss: 19.2493\n",
            "Epoch 7/50\n",
            "357/357 [==============================] - 339s 950ms/step - loss: 16.4438\n",
            "Epoch 8/50\n",
            "357/357 [==============================] - 341s 956ms/step - loss: 14.3859\n",
            "Epoch 9/50\n",
            "357/357 [==============================] - 343s 960ms/step - loss: 13.0039\n",
            "Epoch 10/50\n",
            "357/357 [==============================] - 343s 961ms/step - loss: 11.7843\n",
            "Epoch 11/50\n",
            "357/357 [==============================] - 346s 970ms/step - loss: 10.9311\n",
            "Epoch 12/50\n",
            "357/357 [==============================] - 341s 956ms/step - loss: 10.5168\n",
            "Epoch 13/50\n",
            "357/357 [==============================] - 351s 982ms/step - loss: 9.7483\n",
            "Epoch 14/50\n",
            "357/357 [==============================] - 342s 958ms/step - loss: 9.3315\n",
            "Epoch 15/50\n",
            "357/357 [==============================] - 360s 1s/step - loss: 8.8699\n",
            "Epoch 16/50\n",
            "357/357 [==============================] - 340s 955ms/step - loss: 8.6082\n",
            "Epoch 17/50\n",
            "357/357 [==============================] - 341s 956ms/step - loss: 8.3290\n",
            "Epoch 18/50\n",
            "357/357 [==============================] - 346s 971ms/step - loss: 8.1418\n",
            "Epoch 19/50\n",
            "357/357 [==============================] - 350s 979ms/step - loss: 7.9912\n",
            "Epoch 20/50\n",
            "357/357 [==============================] - 340s 955ms/step - loss: 7.7893\n",
            "Epoch 21/50\n",
            "357/357 [==============================] - 344s 964ms/step - loss: 7.6444\n",
            "Epoch 22/50\n",
            "357/357 [==============================] - 348s 976ms/step - loss: 7.5782\n",
            "Epoch 23/50\n",
            "357/357 [==============================] - 342s 959ms/step - loss: 7.4813\n",
            "Epoch 24/50\n",
            "357/357 [==============================] - 345s 968ms/step - loss: 7.2233\n",
            "Epoch 25/50\n",
            "357/357 [==============================] - 345s 966ms/step - loss: 7.1615\n",
            "Epoch 26/50\n",
            "357/357 [==============================] - 341s 954ms/step - loss: 7.0850\n",
            "Epoch 27/50\n",
            "357/357 [==============================] - 336s 941ms/step - loss: 6.9636\n",
            "Epoch 28/50\n",
            "357/357 [==============================] - 340s 954ms/step - loss: 6.9798\n",
            "Epoch 29/50\n",
            "357/357 [==============================] - 332s 929ms/step - loss: 6.9044\n",
            "Epoch 30/50\n",
            "357/357 [==============================] - 335s 938ms/step - loss: 6.8092\n",
            "Epoch 31/50\n",
            "357/357 [==============================] - 345s 966ms/step - loss: 6.8661\n",
            "Epoch 32/50\n",
            "357/357 [==============================] - 335s 939ms/step - loss: 6.8609\n",
            "Epoch 33/50\n",
            "357/357 [==============================] - 330s 927ms/step - loss: 6.6881\n",
            "Epoch 34/50\n",
            "357/357 [==============================] - 346s 971ms/step - loss: 6.5937\n",
            "Epoch 35/50\n",
            "357/357 [==============================] - 342s 958ms/step - loss: 6.5580\n",
            "Epoch 36/50\n",
            "357/357 [==============================] - 342s 958ms/step - loss: 6.5643\n",
            "Epoch 37/50\n",
            "357/357 [==============================] - 344s 964ms/step - loss: 6.5574\n",
            "Epoch 38/50\n",
            "357/357 [==============================] - 340s 952ms/step - loss: 6.5736\n",
            "Epoch 39/50\n",
            "357/357 [==============================] - 329s 922ms/step - loss: 6.3914\n",
            "Epoch 40/50\n",
            "357/357 [==============================] - 351s 983ms/step - loss: 6.4305\n",
            "Epoch 41/50\n",
            "357/357 [==============================] - 347s 972ms/step - loss: 6.4170\n",
            "Epoch 42/50\n",
            "357/357 [==============================] - 346s 969ms/step - loss: 6.5038\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 43/50\n",
            "357/357 [==============================] - 338s 946ms/step - loss: 5.8068\n",
            "Epoch 44/50\n",
            "357/357 [==============================] - 346s 969ms/step - loss: 5.7411\n",
            "Epoch 45/50\n",
            "357/357 [==============================] - 339s 951ms/step - loss: 5.6928\n",
            "Epoch 46/50\n",
            "357/357 [==============================] - 337s 945ms/step - loss: 5.6081\n",
            "Epoch 47/50\n",
            "357/357 [==============================] - 334s 936ms/step - loss: 5.6248\n",
            "Epoch 48/50\n",
            "357/357 [==============================] - 334s 933ms/step - loss: 5.6305\n",
            "Epoch 49/50\n",
            "357/357 [==============================] - 341s 955ms/step - loss: 5.6415\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 50/50\n",
            "357/357 [==============================] - 345s 965ms/step - loss: 5.5975\n",
            "Unfreeze all of the layers.\n",
            "Train on 5717 samples, val on 5823 samples, with batch size 4.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 51/50000\n",
            "1429/1429 [==============================] - 1536s 1s/step - loss: 6.2276\n",
            "\n",
            "Epoch end eval mAP on weight logs/000/ep050-loss5.597.h5\n",
            "Epoch 51 mAP 0.09769709155624647\n",
            "\n",
            "Epoch 52/50000\n",
            "1429/1429 [==============================] - 1456s 1s/step - loss: 6.3728\n",
            "\n",
            "Epoch end eval mAP on weight logs/000/ep050-loss5.597.h5\n",
            "Epoch 52 mAP 0.09769709155624647\n",
            "\n",
            "Epoch 53/50000\n",
            "1429/1429 [==============================] - 1447s 1s/step - loss: 6.2527\n",
            "\n",
            "Epoch end eval mAP on weight logs/000/ep050-loss5.597.h5\n",
            "Epoch 53 mAP 0.09769709155624647\n",
            "\n",
            "Epoch 54/50000\n",
            " 324/1429 [=====>........................] - ETA: 18:38 - loss: 6.0620"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrv7TlZp9L6h"
      },
      "source": [
        "**추가 과제**  \n",
        "`predict_video.py` 파일을 이용하여, 동영상 인식 및 저장 기능이 있으니,  \n",
        "video 파일의 객체 인식 결과를 확인해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Mish(Layer):\n",
        "    '''\n",
        "    Mish Activation Function.\n",
        "    .. math::\n",
        "        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n",
        "    Shape:\n",
        "        - Input: Arbitrary. Use the keyword argument `input_shape`\n",
        "        (tuple of integers, does not include the samples axis)\n",
        "        when using this layer as the first layer in a model.\n",
        "        - Output: Same shape as the input.\n",
        "    Examples:\n",
        "        >>> X_input = Input(input_shape)\n",
        "        >>> X = Mish()(X_input)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Mish, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs * K.tanh(K.softplus(inputs))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Mish, self).get_config()\n",
        "        return config\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ],
      "metadata": {
        "id": "1lBFdsy8DkAW"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Foynkl4l9kRq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f565da04-7022-4f90-f89b-86318f5568a3"
      },
      "source": [
        "from keras.layers.pooling import GlobalAveragePooling2D\n",
        "from keras.datasets import mnist\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Input, GlobalAveragePooling2D, BatchNormalization, Conv2D, MaxPooling2D, Activation, Dense, Flatten, Reshape\n",
        "from keras.models import Model\n",
        "from keras.optimizer_v2.adam import Adam\n",
        "\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "x_train.shape, y_train.shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = Input(shape=(28, 28))\n",
        "x = Reshape(input_shape=(28, 28), target_shape=(28, 28, 1))(inputs)\n",
        "x = Conv2D(32, (5, 5), padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Mish()(x)\n",
        "x = MaxPooling2D()(x)\n",
        "\n",
        "x = Conv2D(64, (3, 3), padding='valid')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Mish()(x)\n",
        "x = MaxPooling2D()(x)\n",
        "\n",
        "x = Conv2D(5, (3, 3), 1, padding='valid')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Mish()(x)\n",
        "x = MaxPooling2D()(x)\n",
        "\n",
        "x = Conv2D(1, (2, 2), 1)(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "# x = Flatten()(x)\n",
        "outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwF9idNwAGBW",
        "outputId": "7e83d67f-ff0e-4bb1-839e-bfb350f5af03"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_13 (InputLayer)       [(None, 28, 28)]          0         \n",
            "                                                                 \n",
            " reshape_15 (Reshape)        (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            " conv2d_43 (Conv2D)          (None, 28, 28, 32)        832       \n",
            "                                                                 \n",
            " batch_normalization_28 (Bat  (None, 28, 28, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " mish_22 (Mish)              (None, 28, 28, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d_26 (MaxPoolin  (None, 14, 14, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_44 (Conv2D)          (None, 12, 12, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_29 (Bat  (None, 12, 12, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " mish_23 (Mish)              (None, 12, 12, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_27 (MaxPoolin  (None, 6, 6, 64)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_45 (Conv2D)          (None, 4, 4, 5)           2885      \n",
            "                                                                 \n",
            " batch_normalization_30 (Bat  (None, 4, 4, 5)          20        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " mish_24 (Mish)              (None, 4, 4, 5)           0         \n",
            "                                                                 \n",
            " max_pooling2d_28 (MaxPoolin  (None, 2, 2, 5)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_46 (Conv2D)          (None, 1, 1, 1)           21        \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 1)                 0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 10)                20        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,658\n",
            "Trainable params: 22,456\n",
            "Non-trainable params: 202\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28)),\n",
        "        Reshape(input_shape=(28, 28), target_shape=(28, 28, 1)),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3)),\n",
        "     BatchNormalization(),\n",
        "     Mish(),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "     \n",
        "        layers.Conv2D(64, kernel_size=(3, 3)),\n",
        "     BatchNormalization(),\n",
        "     Mish(),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "     \n",
        "        Conv2D(1, kernel_size=(3, 3)),\n",
        "        # layers.GlobalAveragePooling2D(),\n",
        "     Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(10, activation=\"softmax\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "xSm3ZQ8SHsYX"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "zDX9j9yZIEWx"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, \n",
        "                    batch_size=64, epochs=12, \n",
        "                    validation_data=(x_test, y_test), validation_split=0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHfpr5fOIHJR",
        "outputId": "b49258cf-09ac-4b6d-9752-8ae0a7760a7b"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "657/657 [==============================] - 5s 6ms/step - loss: 1.3586 - accuracy: 0.5080 - val_loss: 0.4453 - val_accuracy: 0.8871\n",
            "Epoch 2/12\n",
            "657/657 [==============================] - 4s 7ms/step - loss: 0.8252 - accuracy: 0.6974 - val_loss: 0.3566 - val_accuracy: 0.8920\n",
            "Epoch 3/12\n",
            "657/657 [==============================] - 4s 7ms/step - loss: 0.7194 - accuracy: 0.7300 - val_loss: 0.2588 - val_accuracy: 0.9323\n",
            "Epoch 4/12\n",
            "657/657 [==============================] - 4s 7ms/step - loss: 0.6758 - accuracy: 0.7456 - val_loss: 0.2129 - val_accuracy: 0.9446\n",
            "Epoch 5/12\n",
            "657/657 [==============================] - 4s 6ms/step - loss: 0.6509 - accuracy: 0.7546 - val_loss: 0.1950 - val_accuracy: 0.9476\n",
            "Epoch 6/12\n",
            "657/657 [==============================] - 4s 6ms/step - loss: 0.6279 - accuracy: 0.7630 - val_loss: 0.1848 - val_accuracy: 0.9488\n",
            "Epoch 7/12\n",
            "657/657 [==============================] - 4s 7ms/step - loss: 0.6083 - accuracy: 0.7680 - val_loss: 0.2181 - val_accuracy: 0.9359\n",
            "Epoch 8/12\n",
            "657/657 [==============================] - 4s 6ms/step - loss: 0.5937 - accuracy: 0.7739 - val_loss: 0.2055 - val_accuracy: 0.9406\n",
            "Epoch 9/12\n",
            "657/657 [==============================] - 4s 6ms/step - loss: 0.5841 - accuracy: 0.7797 - val_loss: 0.1754 - val_accuracy: 0.9513\n",
            "Epoch 10/12\n",
            "657/657 [==============================] - 4s 6ms/step - loss: 0.5772 - accuracy: 0.7782 - val_loss: 0.1712 - val_accuracy: 0.9504\n",
            "Epoch 11/12\n",
            "657/657 [==============================] - 4s 7ms/step - loss: 0.5643 - accuracy: 0.7832 - val_loss: 0.1581 - val_accuracy: 0.9555\n",
            "Epoch 12/12\n",
            "657/657 [==============================] - 4s 6ms/step - loss: 0.5573 - accuracy: 0.7891 - val_loss: 0.1647 - val_accuracy: 0.9527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.predict(x[0])\n",
        "\n",
        "np.argmax(result)"
      ],
      "metadata": {
        "id": "uYQfENckEDZx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}