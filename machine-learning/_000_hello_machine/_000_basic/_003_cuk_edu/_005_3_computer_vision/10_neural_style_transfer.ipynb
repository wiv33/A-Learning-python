{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wiv33/A-Learning-python/blob/master/machine-learning/_000_hello_machine/_000_basic/_003_cuk_edu/_005_3_computer_vision/10_neural_style_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n",
    "\n",
    " #2nd method\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "[name: \"/device:CPU:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 11148363520091906334\n xla_global_id: -1,\n name: \"/device:GPU:0\"\n device_type: \"GPU\"\n memory_limit: 5726273536\n locality {\n   bus_id: 1\n   links {\n   }\n }\n incarnation: 16500826804086461567\n physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n xla_global_id: 416903419]"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: memory_profiler in c:\\users\\wivps\\anaconda3\\envs\\tensor\\lib\\site-packages (0.60.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\wivps\\anaconda3\\envs\\tensor\\lib\\site-packages (from memory_profiler) (5.9.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install memory_profiler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "peak memory: 928.16 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler\n",
    "%memit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# print(gpus)\n",
    "# if gpus:\n",
    "#   # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
    "#   try:\n",
    "#     tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "#   except RuntimeError as e:\n",
    "#     # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "#     print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1675778684.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Input \u001B[1;32mIn [70]\u001B[1;36m\u001B[0m\n\u001B[1;33m    **Conv filter 시각화**\u001B[0m\n\u001B[1;37m    ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install jupyterlab-system-monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# print(gpus)\n",
    "# if gpus:\n",
    "#   # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
    "#   try:\n",
    "#     tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "#   except RuntimeError as e:\n",
    "#     # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_8_qDRYXDw9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Inside the conv layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0awbQZGgwBO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Conv filter 시각화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rKj41ov1Z4uw",
    "outputId": "01a995c1-5f6f-4ffe-c8bf-5bdfe6fcfb60",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# summarize filters in each convolutional layer\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load the model\n",
    "model = VGG16()\n",
    "model.summary()\n",
    "# summarize filter shapes\n",
    "for layer in model.layers:\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    # get filter weights\n",
    "    filters, biases = layer.get_weights()\n",
    "    print(layer.name, filters.shape)\n",
    "\n",
    "# retrieve weights from the first hidden layer\n",
    "layer_num = 2\n",
    "filters, biases = model.layers[layer_num].get_weights()\n",
    "print(model.layers[layer_num].name)\n",
    "\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "\n",
    "# plot first few filters\n",
    "n_filters, ix = 6, 1\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(3): # RGB\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(f[:, :, j], cmap='gray')\n",
    "        ix += 1\n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "L1kyUeZRbgOH",
    "outputId": "67065079-9903-4096-f01a-421e44c53d02",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import image as mp_image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/kotech1/computervision/master/img/2011_002300-dog.jpg'\n",
    "\n",
    "sample_image_path = keras.utils.get_file(\n",
    "    \"2011_002300-dog.jpg\", url\n",
    ")\n",
    "image = mp_image.imread(sample_image_path)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrqyBPRzg1We",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Activation value 시각화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "id": "k4ivFmDmaSn5",
    "outputId": "ea526092-16b1-4eed-9dbc-d9bbf5984910",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot feature map of first conv layer for given image\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import expand_dims\n",
    "\n",
    "# load the model\n",
    "model = VGG16()\n",
    "\n",
    "layer = 17  # 1 to 17\n",
    "# redefine model to output right after the first hidden layer\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[layer].output)\n",
    "# load the image with the required shape\n",
    "#img = load_img('bird.jpg', target_size=(224, 224))\n",
    "img = load_img(sample_image_path, target_size=(224, 224))\n",
    "# convert the image to an array\n",
    "img = img_to_array(img)\n",
    "# expand dimensions so that it represents a single 'sample'\n",
    "img = expand_dims(img, axis=0)\n",
    "# prepare the image (e.g. scale pixel values for the vgg)\n",
    "img = preprocess_input(img)\n",
    "# get feature map for first hidden layer\n",
    "feature_maps = model.predict(img)\n",
    "# plot all 64 maps in an 8x8 squares\n",
    "square = 8\n",
    "ix = 1\n",
    "offset = 0 # 0, 64, 128 ... \n",
    "plt.figure(figsize=(8,10))\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        #plt.imshow(feature_maps[0, :, :, ix-1+offset], cmap='gray')\n",
    "        img = feature_maps[0, :, :, ix-1+offset]\n",
    "        import numpy as np\n",
    "        img = np.clip(img, -1000, 1000)\n",
    "        #plt.imshow(feature_maps[0, :, :, ix-1+offset])\n",
    "        plt.imshow(img)\n",
    "        ix += 1\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd4JxFfL_A4n",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neural Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2tHFcyC_xno",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import vgg19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWWy0xrUg70Z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**손실함수 scale factors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZC3BqDpcNvZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Weights of the different loss components\n",
    "total_variation_weight = 1e-6\n",
    "style_weight = 1e-6\n",
    "content_weight = 2.5e-8\n",
    "\n",
    "total_variation_weight = 5e-7\n",
    "style_weight = 1e-5\n",
    "content_weight = 2.5e-8\n",
    "\n",
    "#\n",
    "#total_variation_weight = 1e-8\n",
    "#style_weight = 5e-5 #generated6\n",
    "#content_weight = 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztu4OxF-hAE3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "C: base_image  \n",
    "S: style_reference_image  \n",
    "G: combined_image  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1xVSK1VCMHk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_image_path = keras.utils.get_file(\n",
    "    \"london-bridge.jpg\", \"https://pixy.org/src/0/thumbs350/123.jpg\"\n",
    ")\n",
    "style_reference_image_path = keras.utils.get_file(\n",
    "    \"franz-marc.jpg\", \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Les_Premiers_animaux_de_Franz_Marc_%28Mus%C3%A9e_de_l%27Orangerie%2C_Paris%29_%2840833051033%29.jpg/691px-Les_Premiers_animaux_de_Franz_Marc_%28Mus%C3%A9e_de_l%27Orangerie%2C_Paris%29_%2840833051033%29.jpg\"\n",
    ")\n",
    "result_prefix = 'london_generated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "qeJQoDftIjBV",
    "outputId": "97f481d0-8d24-4730-a8b6-71cebffe36fd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import image as mp_image\n",
    "import matplotlib.pyplot as plt\n",
    "img = mp_image.imread(base_image_path)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "img = mp_image.imread(style_reference_image_path)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDKkrXaDhUbw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "세로 방향을 400px로, aspect ratio를 유지하며 출력 이미지 크기 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PxwDxyzAKEN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dimensions of the generated picture.\n",
    "width, height = keras.preprocessing.image.load_img(base_image_path).size\n",
    "img_nrows = 800\n",
    "img_ncols = int(width * img_nrows / height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqfkUy3Mhbux",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "VGG19의 image level normalization, de-normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3uNm7OeaAYP-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    # Util function to open, resize and format pictures into appropriate tensors\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        image_path, target_size=(img_nrows, img_ncols)\n",
    "    )\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return tf.convert_to_tensor(img)\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Util function to convert a tensor into a valid image\n",
    "    x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sAfmVlKhjba",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Gram matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pNYt19pv4VD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 이미지 텐서의 gram matrix (특징별 outer product)\n",
    "\n",
    "def gram_matrix(x):\n",
    "    # 채널을 맨 앞으로, w,h를 뒤로 보냄\n",
    "    x = tf.transpose(x, (2, 0, 1)) \n",
    "    # (c, h, w) ==> (c, w*h)\n",
    "    features = tf.reshape(x, (tf.shape(x)[0], -1)) \n",
    "    # (c, w*h) x (w*h,c) = (c,c) \n",
    "    gram = tf.matmul(features, tf.transpose(features))  \n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOHuZv9lhoY7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Style Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gA98rBiBdOXg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# style loss는 reference image와 generated image간의 \n",
    "# 스타일 차이를 유지할 수 있도록 하는 역할을 한다.\n",
    "# gram matrix의 차이의 squared error로 정의\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    # 논문의 정확한 implementation은 다음과 같아야 함\n",
    "    # channels = style.shape[2]\n",
    "    # size = style.shape[0] * style.shape[1] \n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQBVJB96huQl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Content Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvEPa6lBZ02L",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 콘텐츠 손실 함수\n",
    "# 생성된 이미지에서 base 이미지의 \"content\"를\n",
    "# 유지하기 위함\n",
    "\n",
    "def content_loss(base, combination):\n",
    "    return tf.reduce_sum(tf.square(combination - base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y65rBdK4hxo8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Total variation loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ER9qPjkZPecy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 3번째 손실함수: total variation loss\n",
    "# designed to keep the generated image locally coherent\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    a = tf.square(\n",
    "        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, 1:, : img_ncols - 1, :]\n",
    "    )\n",
    "    b = tf.square(\n",
    "        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, : img_nrows - 1, 1:, :]\n",
    "    )\n",
    "    return tf.reduce_sum(tf.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rwdmEA3h2kc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**VGG19**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qf0LL5N3hXTt",
    "outputId": "6315a0f9-a1ab-4b2e-c15b-1ce6ce0af519",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# VGG19 model 로딩 (pre-trained ImageNet weights)\n",
    "model = vgg19.VGG19(weights=\"imagenet\", include_top=False)\n",
    "model.summary()\n",
    "\n",
    "# 모델에서 레이어 이름과 레이어 출력 형식을 가져옴\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "\n",
    "# VGG 모델의 전체 activation value를 출력하는 함수 정의\n",
    "# (dict 형식)\n",
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VCgYSxBa3nu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# style loss에 사용할 레이어\n",
    "#style_layer_names = [\n",
    "#    \"block1_conv1\",\n",
    "#    \"block2_conv1\",\n",
    "#    \"block3_conv1\",\n",
    "#    \"block4_conv1\",\n",
    "#    \"block5_conv1\",\n",
    "#]\n",
    "\n",
    "style_layer_names = [\n",
    "    \"block1_conv2\",\n",
    "    \"block2_conv2\",\n",
    "    \"block3_conv2\",\n",
    "    \"block4_conv2\",\n",
    "    \"block5_conv2\",\n",
    "]\n",
    "\n",
    "# content loss에 사용할 레이어\n",
    "content_layer_name = \"block5_conv2\"\n",
    "\n",
    "def compute_loss(combination_image, base_image, style_reference_image):\n",
    "    input_tensor = tf.concat(\n",
    "        [base_image, style_reference_image, combination_image], axis=0\n",
    "    )\n",
    "    features = feature_extractor(input_tensor)\n",
    "\n",
    "    # loss 초기화\n",
    "    loss = tf.zeros(shape=())\n",
    "\n",
    "    # content loss\n",
    "    layer_features = features[content_layer_name]\n",
    "    base_image_features = layer_features[0, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    c_loss = content_weight * content_loss(\n",
    "        base_image_features, combination_features\n",
    "    )\n",
    "    s_loss = tf.zeros(shape=())\n",
    "    # Add style loss\n",
    "    for layer_name in style_layer_names:\n",
    "        layer_features = features[layer_name]\n",
    "        style_reference_features = layer_features[1, :, :, :]\n",
    "        combination_features = layer_features[2, :, :, :]\n",
    "        sl = style_loss(style_reference_features, combination_features)\n",
    "        s_loss += (style_weight / len(style_layer_names)) * sl\n",
    "    \n",
    "    # total variation loss\n",
    "    t_loss = total_variation_weight * total_variation_loss(combination_image)\n",
    "    loss = c_loss+s_loss+t_loss \n",
    "    return loss, c_loss, s_loss, t_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_l_U0L8aui0Z",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lQz334qYnsD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 여러 복잡한 연산 실행히 빠른 실행을 위한 사전 컴파일\n",
    "@tf.function\n",
    "def compute_loss_and_grads(combination_image, base_image, style_reference_image):\n",
    "    # 안에서 일어나는 연산을 gradient를 추적할 수 있음.\n",
    "    # loss를 계산할 때는 gradientTape 안에서 계산해야함.\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, c_loss, s_loss, t_loss = compute_loss(combination_image, base_image, style_reference_image)\n",
    "    grads = tape.gradient(loss, combination_image)\n",
    "    return loss, grads, c_loss, s_loss, t_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9qhZG5cEegy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**다음의 학습 코드를 수행하는 데에는 약3시간 정도 소요됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krFR92dIcaEH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(\n",
    "    keras.optimizers.schedules.ExponentialDecay( # 트레이닝이 길어질 때 learning rate를 점점 줄여주도록 하는 deco\n",
    "        initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96\n",
    "    )\n",
    ")\n",
    "\n",
    "base_image = preprocess_image(base_image_path)\n",
    "style_reference_image = preprocess_image(style_reference_image_path)\n",
    "combination_image = tf.Variable(preprocess_image(base_image_path))\n",
    "\n",
    "iterations = 1200\n",
    "for i in range(1, iterations + 1):\n",
    "    loss, grads, c_loss, s_loss, t_loss = compute_loss_and_grads(\n",
    "        combination_image, base_image, style_reference_image\n",
    "    )\n",
    "    optimizer.apply_gradients([(grads, combination_image)])\n",
    "    if i % 10 == 0 or i == 1:\n",
    "        print(\"Iteration %d: loss=%.2f, content_loss=%.2f, style_loss=%.2f, total_variation_loss=%.2f\" %\n",
    "               (i, loss, c_loss, s_loss, t_loss))\n",
    "    if i % 100 == 0:\n",
    "        img = deprocess_image(combination_image.numpy())\n",
    "        fname = result_prefix + \"_at_iteration_%d.png\" % i\n",
    "        keras.preprocessing.image.save_img(fname, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "FJkSYfbpU4bS",
    "outputId": "2be50758-6320-482d-94a5-b3fb62563df6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "img = mp_image.imread(result_prefix + '_at_iteration_'+'%d.png' % 1300)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "IWCbdCQoYtUm",
    "outputId": "c2d5174c-11ed-46aa-dbbe-ee635d06df01",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "img = mp_image.imread(result_prefix + '_at_iteration_'+'%d.png' % 100)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOy7ZGnrE3vx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSBbOb9KuboQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import vgg19\n",
    "import os\n",
    "\n",
    "def preprocess_image(image_path, img_nrows, img_ncols):\n",
    "    # Util function to open, resize and format pictures into appropriate tensors\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        image_path, target_size=(img_nrows, img_ncols)\n",
    "    )\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return tf.convert_to_tensor(img)\n",
    "\n",
    "\n",
    "def deprocess_image(x, img_nrows, img_ncols):\n",
    "    # Util function to convert a tensor into a valid image\n",
    "    x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x\n",
    "\n",
    "# 이미지 텐서의 gram matrix (특징별 outer product)\n",
    "\n",
    "def gram_matrix(x):\n",
    "    # 채널을 맨 앞으로, w,h를 뒤로 보냄\n",
    "    x = tf.transpose(x, (2, 0, 1)) \n",
    "    # (c, h, w) ==> (c, w*h)\n",
    "    features = tf.reshape(x, (tf.shape(x)[0], -1)) \n",
    "    # (c, w*h) x (w*h,c) = (c,c) \n",
    "    gram = tf.matmul(features, tf.transpose(features))  \n",
    "    return gram\n",
    "\n",
    "# style loss는 reference image와 generated image간의 \n",
    "# 스타일 차이를 유지할 수 있도록 하는 역할을 한다.\n",
    "# gram matrix의 차이의 squared error로 정의\n",
    "\n",
    "def style_loss(style, combination, img_nrows, img_ncols):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    # 논문의 정확한 implementation은 다음과 같아야 함\n",
    "    # channels = style.shape[2]\n",
    "    # size = style.shape[0] * style.shape[1] \n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n",
    "\n",
    "# 콘텐츠 손실 함수\n",
    "# 생성된 이미지에서 base 이미지의 \"content\"를\n",
    "# 유지하기 위함\n",
    "\n",
    "def content_loss(base, combination):\n",
    "    return tf.reduce_sum(tf.square(combination - base))\n",
    "\n",
    "# 3번째 손실함수: total variation loss\n",
    "# designed to keep the generated image locally coherent\n",
    "\n",
    "def total_variation_loss(x, img_nrows, img_ncols):\n",
    "    a = tf.square(\n",
    "        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, 1:, : img_ncols - 1, :]\n",
    "    )\n",
    "    b = tf.square(\n",
    "        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, : img_nrows - 1, 1:, :]\n",
    "    )\n",
    "    return tf.reduce_sum(tf.pow(a + b, 1.25))\n",
    "\n",
    "\n",
    "\n",
    "def compute_loss(feature_extractor, combination_image, base_image, style_reference_image, selected_style_layer_names, content_layer_name, total_variation_weight, style_weight, content_weight, img_nrows, img_ncols):\n",
    "    input_tensor = tf.concat(\n",
    "        [base_image, style_reference_image, combination_image], axis=0\n",
    "    )\n",
    "    features = feature_extractor(input_tensor)\n",
    "\n",
    "    # loss 초기화\n",
    "    loss = tf.zeros(shape=())\n",
    "\n",
    "    # content loss\n",
    "    layer_features = features[content_layer_name]\n",
    "    base_image_features = layer_features[0, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    c_loss = content_weight * content_loss(\n",
    "        base_image_features, combination_features\n",
    "    )\n",
    "    s_loss = tf.zeros(shape=())\n",
    "    # Add style loss\n",
    "    for layer_name in selected_style_layer_names:\n",
    "        layer_features = features[layer_name]\n",
    "        style_reference_features = layer_features[1, :, :, :]\n",
    "        combination_features = layer_features[2, :, :, :]\n",
    "        sl = style_loss(style_reference_features, combination_features, img_nrows, img_ncols)\n",
    "        s_loss += (style_weight / len(selected_style_layer_names)) * sl\n",
    "    \n",
    "    # total variation loss\n",
    "    t_loss = total_variation_weight * total_variation_loss(combination_image, img_nrows, img_ncols)\n",
    "    loss = c_loss+s_loss+t_loss \n",
    "    return loss, c_loss, s_loss, t_loss\n",
    "\n",
    "# 여러 복잡한 연산 실행히 빠른 실행을 위한 사전 컴파일\n",
    "@tf.function\n",
    "def compute_loss_and_grads(feature_extractor, combination_image, base_image, style_reference_image, selected_style_layer_names, content_layer_name, total_variation_weight, style_weight, content_weight, img_nrows, img_ncols):\n",
    "    # 안에서 일어나는 연산을 gradient를 추적할 수 있음.\n",
    "    # loss를 계산할 때는 gradientTape 안에서 계산해야함.\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, c_loss, s_loss, t_loss = compute_loss(feature_extractor, combination_image, base_image, style_reference_image, selected_style_layer_names, content_layer_name, total_variation_weight, style_weight, content_weight, img_nrows, img_ncols)\n",
    "    grads = tape.gradient(loss, combination_image)\n",
    "    return loss, grads, c_loss, s_loss, t_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _main_call(_version_num, _base_img_name, _style_img_name, _style_layer_version, _style_layer_num):\n",
    "    import platform\n",
    "    # Weights of the different loss components\n",
    "    total_variation_weight = 1e-6\n",
    "    style_weight = 1e-6\n",
    "    content_weight = 2.5e-8\n",
    "\n",
    "    if _version_num == 2:\n",
    "        total_variation_weight = 5e-7\n",
    "        style_weight = 1e-5\n",
    "        content_weight = 2.5e-8\n",
    "    elif _version_num == 3:\n",
    "        total_variation_weight = 1e-8\n",
    "        style_weight = 5e-5 #generated6\n",
    "        content_weight = 1e-10\n",
    "\n",
    "    root_dir_path = '/content/drive/MyDrive/Colab Notebooks/data/data/gan_img_result'\n",
    "    file_ext = 'jpeg'\n",
    "\n",
    "    is_windows = platform.system().lower().__contains__(\"windows\")\n",
    "    if is_windows:\n",
    "        root_dir_path = 'C:\\\\Users\\\\wivps\\\\Documents\\\\카카오톡 받은 파일\\\\transfer_image'\n",
    "        file_ext = 'jpg'\n",
    "\n",
    "\n",
    "\n",
    "    base_img_name = _base_img_name\n",
    "    base_image_path = f\"{root_dir_path}/{base_img_name}.{file_ext}\"\n",
    "    style_img_name = _style_img_name\n",
    "    style_reference_image_path = f'{root_dir_path}/{style_img_name}.{file_ext}'\n",
    "\n",
    "    img_version = 'v%d_%d_%d' % (_version_num, _style_layer_version, _style_layer_num)\n",
    "    result_prefix = f'{root_dir_path}/{img_version}'\n",
    "\n",
    "    from matplotlib import image as mp_image\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    img = mp_image.imread(base_image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    img = mp_image.imread(style_reference_image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Dimensions of the generated picture.\n",
    "    width, height = keras.preprocessing.image.load_img(base_image_path).size\n",
    "    img_nrows = 300\n",
    "    img_ncols = int(width * img_nrows / height)\n",
    "\n",
    "    # VGG19 model 로딩 (pre-trained ImageNet weights)\n",
    "    model = vgg19.VGG19(weights=\"imagenet\", include_top=False)\n",
    "    model.summary()\n",
    "\n",
    "    # 모델에서 레이어 이름과 레이어 출력 형식을 가져옴\n",
    "    outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "\n",
    "    # VGG 모델의 전체 activation value를 출력하는 함수 정의\n",
    "    # (dict 형식)\n",
    "    feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)\n",
    "\n",
    "    # style loss에 사용할 레이어\n",
    "    style_layer_names = ([\n",
    "    \"block1_conv1\",\n",
    "    \"block2_conv1\",\n",
    "    \"block3_conv1\",\n",
    "    \"block4_conv1\",\n",
    "    \"block5_conv1\",\n",
    "    ],[\n",
    "        \"block1_conv2\",\n",
    "        \"block2_conv2\",\n",
    "        \"block3_conv2\",\n",
    "        \"block4_conv2\",\n",
    "        \"block5_conv2\",\n",
    "    ])\n",
    "\n",
    "\n",
    "    # content loss에 사용할 레이어\n",
    "    selected_style_layer_names = style_layer_names[_style_layer_version]\n",
    "    content_layer_name = selected_style_layer_names[_style_layer_num]\n",
    "    print(content_layer_name)\n",
    "\n",
    "    optimizer = keras.optimizers.SGD(\n",
    "        keras.optimizers.schedules.ExponentialDecay( # 트레이닝이 길어질 때 learning rate를 점점 줄여주도록 하는 deco\n",
    "            initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96\n",
    "        )\n",
    "    )\n",
    "\n",
    "    base_image = preprocess_image(base_image_path, img_nrows, img_ncols)\n",
    "    style_reference_image = preprocess_image(style_reference_image_path, img_nrows, img_ncols)\n",
    "    combination_image = tf.Variable(preprocess_image(base_image_path, img_nrows, img_ncols))\n",
    "\n",
    "    # make dirs\n",
    "    make_dir = f'{result_prefix}/{base_img_name}_{style_img_name}'\n",
    "    # os.makedirs(make_dir, exist_ok=False)\n",
    "    if not os.path.exists(make_dir):\n",
    "        os.makedirs(make_dir)\n",
    "\n",
    "    iterations = 1200\n",
    "    for i in range(0, iterations + 1):\n",
    "        loss, grads, c_loss, s_loss, t_loss = compute_loss_and_grads(\n",
    "            feature_extractor, combination_image, base_image, style_reference_image, selected_style_layer_names, content_layer_name, total_variation_weight, style_weight, content_weight, img_nrows, img_ncols\n",
    "        )\n",
    "        optimizer.apply_gradients([(grads, combination_image)])\n",
    "        if i % 10 == 0 or i == 1:\n",
    "            print(\"Iteration %d: loss=%.2f, content_loss=%.2f, style_loss=%.2f, total_variation_loss=%.2f\" %\n",
    "                (i, loss, c_loss, s_loss, t_loss))\n",
    "        if (i < 30 and i % 5 == 0) or i % 100 == 0:\n",
    "            img = deprocess_image(combination_image.numpy(), img_nrows, img_ncols)\n",
    "            fname = f'{make_dir}/at_iteration_{i}.png'\n",
    "            keras.preprocessing.image.save_img(fname, img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EwC3J9R9uy2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### style_002.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxtBpZPc9ECE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_main_call(1, 'content_001', '002', 0, 0)\n",
    "_main_call(1, 'content_001', '002', 0, 1)\n",
    "_main_call(1, 'content_001', '002', 0, 2)\n",
    "_main_call(1, 'content_001', '002', 0, 3)\n",
    "_main_call(1, 'content_001', '002', 0, 4)\n",
    "\n",
    "_main_call(1, 'content_001', '002', 1, 0)\n",
    "_main_call(1, 'content_001', '002', 1, 1)\n",
    "_main_call(1, 'content_001', '002', 1, 2)\n",
    "_main_call(1, 'content_001', '002', 1, 3)\n",
    "_main_call(1, 'content_001', '002', 1, 4)\n",
    "\n",
    "_main_call(2, 'content_001', '002', 0, 0)\n",
    "_main_call(2, 'content_001', '002', 0, 1)\n",
    "_main_call(2, 'content_001', '002', 0, 2)\n",
    "_main_call(2, 'content_001', '002', 0, 3)\n",
    "_main_call(2, 'content_001', '002', 0, 4)\n",
    "\n",
    "_main_call(2, 'content_001', '002', 1, 0)\n",
    "_main_call(2, 'content_001', '002', 1, 1)\n",
    "_main_call(2, 'content_001', '002', 1, 2)\n",
    "_main_call(2, 'content_001', '002', 1, 3)\n",
    "_main_call(2, 'content_001', '002', 1, 4)\n",
    "\n",
    "_main_call(3, 'content_001', '002', 0, 0)\n",
    "_main_call(3, 'content_001', '002', 0, 1)\n",
    "_main_call(3, 'content_001', '002', 0, 2)\n",
    "_main_call(3, 'content_001', '002', 0, 3)\n",
    "_main_call(3, 'content_001', '002', 0, 4)\n",
    "\n",
    "_main_call(3, 'content_001', '002', 1, 0)\n",
    "_main_call(3, 'content_001', '002', 1, 1)\n",
    "_main_call(3, 'content_001', '002', 1, 2)\n",
    "_main_call(3, 'content_001', '002', 1, 3)\n",
    "_main_call(3, 'content_001', '002', 1, 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PadJw_jE9tmR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### style_003.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-LMPjq39EpR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_main_call(1, 'content_001', '003', 0, 0)\n",
    "_main_call(1, 'content_001', '003', 0, 1)\n",
    "_main_call(1, 'content_001', '003', 0, 2)\n",
    "_main_call(1, 'content_001', '003', 0, 3)\n",
    "_main_call(1, 'content_001', '003', 0, 4)\n",
    "\n",
    "_main_call(1, 'content_001', '003', 1, 0)\n",
    "_main_call(1, 'content_001', '003', 1, 1)\n",
    "_main_call(1, 'content_001', '003', 1, 2)\n",
    "_main_call(1, 'content_001', '003', 1, 3)\n",
    "_main_call(1, 'content_001', '003', 1, 4)\n",
    "\n",
    "_main_call(2, 'content_001', '003', 0, 0)\n",
    "_main_call(2, 'content_001', '003', 0, 1)\n",
    "_main_call(2, 'content_001', '003', 0, 2)\n",
    "_main_call(2, 'content_001', '003', 0, 3)\n",
    "_main_call(2, 'content_001', '003', 0, 4)\n",
    "\n",
    "_main_call(2, 'content_001', '003', 1, 0)\n",
    "_main_call(2, 'content_001', '003', 1, 1)\n",
    "_main_call(2, 'content_001', '003', 1, 2)\n",
    "_main_call(2, 'content_001', '003', 1, 3)\n",
    "_main_call(2, 'content_001', '003', 1, 4)\n",
    "\n",
    "_main_call(3, 'content_001', '003', 0, 0)\n",
    "_main_call(3, 'content_001', '003', 0, 1)\n",
    "_main_call(3, 'content_001', '003', 0, 2)\n",
    "_main_call(3, 'content_001', '003', 0, 3)\n",
    "_main_call(3, 'content_001', '003', 0, 4)\n",
    "\n",
    "_main_call(3, 'content_001', '003', 1, 0)\n",
    "_main_call(3, 'content_001', '003', 1, 1)\n",
    "_main_call(3, 'content_001', '003', 1, 2)\n",
    "_main_call(3, 'content_001', '003', 1, 3)\n",
    "_main_call(3, 'content_001', '003', 1, 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPF9NHpW9jWO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## content_002\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zep44JJ9zqw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### style_001.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Du4EAEN9hW_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_main_call(1, 'content_002', '001', 0, 0)\n",
    "_main_call(1, 'content_002', '001', 0, 1)\n",
    "_main_call(1, 'content_002', '001', 0, 2)\n",
    "_main_call(1, 'content_002', '001', 0, 3)\n",
    "_main_call(1, 'content_002', '001', 0, 4)\n",
    "\n",
    "_main_call(1, 'content_002', '001', 1, 0)\n",
    "_main_call(1, 'content_002', '001', 1, 1)\n",
    "_main_call(1, 'content_002', '001', 1, 2)\n",
    "_main_call(1, 'content_002', '001', 1, 3)\n",
    "_main_call(1, 'content_002', '001', 1, 4)\n",
    "\n",
    "_main_call(2, 'content_002', '001', 0, 0)\n",
    "_main_call(2, 'content_002', '001', 0, 1)\n",
    "_main_call(2, 'content_002', '001', 0, 2)\n",
    "_main_call(2, 'content_002', '001', 0, 3)\n",
    "_main_call(2, 'content_002', '001', 0, 4)\n",
    "\n",
    "_main_call(2, 'content_002', '001', 1, 0)\n",
    "_main_call(2, 'content_002', '001', 1, 1)\n",
    "_main_call(2, 'content_002', '001', 1, 2)\n",
    "_main_call(2, 'content_002', '001', 1, 3)\n",
    "_main_call(2, 'content_002', '001', 1, 4)\n",
    "\n",
    "_main_call(3, 'content_002', '001', 0, 0)\n",
    "_main_call(3, 'content_002', '001', 0, 1)\n",
    "_main_call(3, 'content_002', '001', 0, 2)\n",
    "_main_call(3, 'content_002', '001', 0, 3)\n",
    "_main_call(3, 'content_002', '001', 0, 4)\n",
    "\n",
    "_main_call(3, 'content_002', '001', 1, 0)\n",
    "_main_call(3, 'content_002', '001', 1, 1)\n",
    "_main_call(3, 'content_002', '001', 1, 2)\n",
    "_main_call(3, 'content_002', '001', 1, 3)\n",
    "_main_call(3, 'content_002', '001', 1, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpYyJ-Q096lJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### style_002.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jirrOneN98pB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_main_call(1, 'content_002', '002', 0, 0)\n",
    "_main_call(1, 'content_002', '002', 0, 1)\n",
    "_main_call(1, 'content_002', '002', 0, 2)\n",
    "_main_call(1, 'content_002', '002', 0, 3)\n",
    "_main_call(1, 'content_002', '002', 0, 4)\n",
    "\n",
    "_main_call(1, 'content_002', '002', 1, 0)\n",
    "_main_call(1, 'content_002', '002', 1, 1)\n",
    "_main_call(1, 'content_002', '002', 1, 2)\n",
    "_main_call(1, 'content_002', '002', 1, 3)\n",
    "_main_call(1, 'content_002', '002', 1, 4)\n",
    "\n",
    "_main_call(2, 'content_002', '002', 0, 0)\n",
    "_main_call(2, 'content_002', '002', 0, 1)\n",
    "_main_call(2, 'content_002', '002', 0, 2)\n",
    "_main_call(2, 'content_002', '002', 0, 3)\n",
    "_main_call(2, 'content_002', '002', 0, 4)\n",
    "\n",
    "_main_call(2, 'content_002', '002', 1, 0)\n",
    "_main_call(2, 'content_002', '002', 1, 1)\n",
    "_main_call(2, 'content_002', '002', 1, 2)\n",
    "_main_call(2, 'content_002', '002', 1, 3)\n",
    "_main_call(2, 'content_002', '002', 1, 4)\n",
    "\n",
    "_main_call(3, 'content_002', '002', 0, 0)\n",
    "_main_call(3, 'content_002', '002', 0, 1)\n",
    "_main_call(3, 'content_002', '002', 0, 2)\n",
    "_main_call(3, 'content_002', '002', 0, 3)\n",
    "_main_call(3, 'content_002', '002', 0, 4)\n",
    "\n",
    "_main_call(3, 'content_002', '002', 1, 0)\n",
    "_main_call(3, 'content_002', '002', 1, 1)\n",
    "_main_call(3, 'content_002', '002', 1, 2)\n",
    "_main_call(3, 'content_002', '002', 1, 3)\n",
    "_main_call(3, 'content_002', '002', 1, 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdxDuIRl9-e-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### style_003.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyvXSVHI9_ee",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_main_call(1, 'content_002', '003', 0, 0)\n",
    "_main_call(1, 'content_002', '003', 0, 1)\n",
    "_main_call(1, 'content_002', '003', 0, 2)\n",
    "_main_call(1, 'content_002', '003', 0, 3)\n",
    "_main_call(1, 'content_002', '003', 0, 4)\n",
    "\n",
    "_main_call(1, 'content_002', '003', 1, 0)\n",
    "_main_call(1, 'content_002', '003', 1, 1)\n",
    "_main_call(1, 'content_002', '003', 1, 2)\n",
    "_main_call(1, 'content_002', '003', 1, 3)\n",
    "_main_call(1, 'content_002', '003', 1, 4)\n",
    "\n",
    "_main_call(2, 'content_002', '003', 0, 0)\n",
    "_main_call(2, 'content_002', '003', 0, 1)\n",
    "_main_call(2, 'content_002', '003', 0, 2)\n",
    "_main_call(2, 'content_002', '003', 0, 3)\n",
    "_main_call(2, 'content_002', '003', 0, 4)\n",
    "\n",
    "_main_call(2, 'content_002', '003', 1, 0)\n",
    "_main_call(2, 'content_002', '003', 1, 1)\n",
    "_main_call(2, 'content_002', '003', 1, 2)\n",
    "_main_call(2, 'content_002', '003', 1, 3)\n",
    "_main_call(2, 'content_002', '003', 1, 4)\n",
    "\n",
    "_main_call(3, 'content_002', '003', 0, 0)\n",
    "_main_call(3, 'content_002', '003', 0, 1)\n",
    "_main_call(3, 'content_002', '003', 0, 2)\n",
    "_main_call(3, 'content_002', '003', 0, 3)\n",
    "_main_call(3, 'content_002', '003', 0, 4)\n",
    "\n",
    "_main_call(3, 'content_002', '003', 1, 0)\n",
    "_main_call(3, 'content_002', '003', 1, 1)\n",
    "_main_call(3, 'content_002', '003', 1, 2)\n",
    "_main_call(3, 'content_002', '003', 1, 3)\n",
    "_main_call(3, 'content_002', '003', 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-TBOAid-_TC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "10_neural_style_transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}